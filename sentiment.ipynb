{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Part 1.:](#part1.) Contents\n",
    "\n",
    "[Part 2.:](#part2.) Introduction\n",
    "\n",
    "____ [Part 2.1.:](#part2.1.) Column Names\n",
    "\n",
    "____ [Part 2.2.:](#part2.2.) Packages\n",
    "\n",
    "____ [Part 2.3.:](#part2.3.) Constants\n",
    "\n",
    "____ [Part 2.4.:](#part2.4.) Read the File\n",
    "\n",
    "[Part 3.:](#part3.) Action\n",
    "\n",
    "____ [Step 3.1.:](#step3.1.) Basic Information\n",
    "\n",
    "________ [Step 3.1.1.:](#step3.1.1.) Investigate the Dataframe\n",
    "\n",
    "________ [Step 3.1.2.:](#step3.1.2.) Investigate the Sentiments\n",
    "\n",
    "____________ [Step 3.1.2.1.:](#step3.1.2.1.) Find Duplicates\n",
    "\n",
    "____________ [Step 3.1.2.2.:](#step3.1.2.2.) Remove Duplicates\n",
    "\n",
    "____________ [Step 3.1.2.3.:](#step3.1.2.3.) Neutralize Sentiments\n",
    "\n",
    "________ [Step 3.1.3.:](#step3.1.3.) Clean Out\n",
    "\n",
    "____ [Step 3.2.:](#step3.2.) Generate Corpus\n",
    "\n",
    "____ [Side Track:](#sidetrackmemoryusage) Memory Usage\n",
    "\n",
    "____ [Step 3.3.:](#step3.3.) The Vocabulary\n",
    "\n",
    "________ [Step 3.3.1.:](#step3.3.1.) Remove Unnecessary Tokens\n",
    "\n",
    "____________ [Step 3.3.1.1.:](#step3.3.1.1.) Remove Hyperlinks\n",
    "\n",
    "____________ [Step 3.3.1.2.:](#step3.3.1.2.) Remove @-tags\n",
    "\n",
    "____________ [Step 3.3.1.2.:](#step3.3.1.3.) Remove #-tags\n",
    "\n",
    "____________ [Step 3.3.1.3.:](#step3.3.1.4.) Remove punctuation\n",
    "\n",
    "____________ [Step 3.3.1.4.:](#step3.3.1.5.) Remove \"stop\" words\n",
    "\n",
    "________________ [Step 3.3.1.4.1.:](#step3.3.1.5.1.) Articles: \"a\", \"an\", \"the\"\n",
    "\n",
    "________________ [Step 3.3.1.4.2.:](#step3.3.1.5.2.) Conjunctions: \"and\", \"or\", \"but\"\n",
    "\n",
    "________________ [Step 3.3.1.4.3.:](#step3.3.1.5.3.) Prepositions: \"at\", \"on\", \"in\", \"of\", \"to\", \"with\", \"by\"\n",
    "\n",
    "________________ [Step 3.3.1.4.4.:](#step3.3.1.5.4.) Pronouns: \"he\", \"she\", \"it\", \"they\", \"we\", \"you\"\n",
    "\n",
    "________________ [Step 3.3.1.4.5.:](#step3.3.1.5.5.) Auxiliary verbs: \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"have\", \"has\", \"had\", \"do\", \n",
    "\"does\", \"did\"\n",
    "\n",
    "________________ [Step 3.3.1.4.6.:](#step3.3.1.5.6.) Adverbs of frequency: \"always\", \"usually\", \"often\", \"sometimes\", \"rarely\", \"never\"\n",
    "\n",
    "________________ [Step 3.3.1.4.7.:](#step3.3.1.5.7.) Interjections: \"oh\", \"ah\", \"wow\", \"hmm\"\n",
    "\n",
    "________________ [Step 3.3.1.4.8.:](#step3.3.1.5.8.) Suggestions from other sources\n",
    "\n",
    "________ [Step 3.3.2.:](#step3.3.2) Gather Unique Tokens\n",
    "\n",
    "____________ [Step 3.3.2.1.:](#step3.3.2.1.)  Save a list of all the unique tokens from the 'corpus' in a csv-file for ease of use; this one we'll call 'token_list'\n",
    "\n",
    "____________ [Step 3.3.2.2.:](#step3.3.2.2.)  Find and add the frequency of each token in the 'corpus' to the 'token_list'\n",
    "\n",
    "________ [Step 3.3.2.3.:](#step3.3.2.3.) Split & Count\n",
    "\n",
    "____________ [Step 3.3.2.4.:](#step3.3.2.4.) Cleaning with Regard to Sentiments\n",
    "\n",
    "________________ [Step 3.3.2.4.1.:](#step3.3.2.4.1.) Look at the three categories of sentiments and try to find what differentiates a positive sentiment from a negative or a neutral one, with regard to the words used in their related texts</li>\n",
    "\n",
    "________________ [Step 3.3.2.4.2.:](#step3.3.2.4.2.) At last, we'll prune the 'text' column to leave only the words that are necessary to keep in each tweet. This is what constitutes the input to our ML system. And we'll save this as another csv-file, together with the corresponding sentiments - the outputs - which we will call the 'column'\n",
    "\n",
    "____ [Step 3.4.:](#step3.4.) Learn from Google's Search Engine\n",
    "\n",
    "________ [Step 3.4.1.:](#step3.4.1.)  Word frequency analysis: This algorithm counts the frequency of each word in a text and identifies the most common words.\n",
    "\n",
    "________ [Step 3.4.2.:](#step3.4.2.)  TF-Idframe: This algorithm identifies the most important words in a text by comparing their frequency in the text to their frequency in a larger corpus of texts.\n",
    "\n",
    "________ [Step 3.4.3.:](#step3.4.3.)  Latent Dirichlet Allocation (LDA): This algorithm is used for topic modeling, which involves identifying the topics present in a text or set of texts.\n",
    "\n",
    "________ [Step 3.4.4.:](#step3.4.4.)  Word2Vec: This algorithm creates a vector representation for each word in a text, which can be used for various NLP tasks like semantic similarity and word analogy.\n",
    "\n",
    "________ [Step 3.4.5.:](#step3.4.5.)  Deep Learning Models: Google also uses deep learning models such as neural networks for various NLP tasks, including language translation, sentiment analysis, and question answering.\n",
    "\n",
    "[Part 4.:](#part4.)  Side Track - Split the Date\n",
    "\n",
    "____ [Step 4.1.:](#step4.1.)  Side Track Follow-Up\n",
    "\n",
    "[Appendix:](#appendix) Test Zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  body {\n",
    "    color: black;\n",
    "  }\n",
    "  h1 {\n",
    "    background-color: transparent;\n",
    "    color: LightSteelBlue;\n",
    "  }\n",
    "  h3 {\n",
    "    background-color: transparent;\n",
    "    color: WhiteSmoke;\n",
    "  }\n",
    "  b {\n",
    "    background-color: transparent;\n",
    "    color: LightSteelBlue;\n",
    "  }\n",
    "</style>\n",
    "<h1><a id='part1.'>Part 1.:</a> Introduction</h1>\n",
    "\n",
    "<h3><a id='part2.1.'>Part 2.1.:</a> Column Names</h3>\n",
    "<ul>\n",
    "  <li><b>ArithmeticErrortarget:</b> the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)</li>\n",
    "  <li><b>id:</b> The id of the tweet (2087)</li>\n",
    "  <li><b>date:</b> The date of the tweet (Sat May 16 23.:58.:44 UTC 2009)</li>\n",
    "  <li><b>flag:</b> The query (lyx). If there is no query, then this value is NO_QUERY.</li>\n",
    "  <li><b>user:</b> The user that tweeted (robotickilldozr)</li>\n",
    "  <li><b>text:</b> The text of the tweet (Lyx is cool)</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id='part2.2'>Part 2.2.:</a> Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import packages as pkg\n",
    "import functions as func\n",
    "import constants as const\n",
    "\n",
    "if __name__ == '__name__':\n",
    "    print('Starting Spark Session. All data is loaded into memory')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id='part2.3.'>Part 2.3.:</a> Constants</h3>\n",
    "\n",
    "Constant values that are used throughout the project are stored in constants.py. This includes the path to the data, the path to the output, and the names of the columns in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id='part2.4.'>Part 2.4.:</a> Read the File</h3>\n",
    "\n",
    "Read the csv file into a Python dataframe and then write it into a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = pkg.pd.read_csv('.databases/tweets.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  body {\n",
    "    color: black;\n",
    "  }\n",
    "  h1 {\n",
    "    background-color: transparent;\n",
    "    color: LightSteelBlue;\n",
    "  }\n",
    "  h3 {\n",
    "    background-color: transparent;\n",
    "    color: WhiteSmoke;\n",
    "  }\n",
    "  b {\n",
    "    background-color: transparent;\n",
    "    color: LightSteelBlue;\n",
    "  }\n",
    "</style>\n",
    "<h1><a id='part3'>Part 3.:</a> Action</h1>\n",
    "\n",
    "<h3><a id=' Step 3.1'>Step 3.1.:</a> Basic Information</h3>\n",
    "\n",
    "Get the basic information about the content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  \\\n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_   \n",
       "0          scotthamilton  \\\n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0           is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "1                                 @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "2                                                                           my whole body feels itchy and like its on fire   \n",
       "3           @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   \n",
       "4                                                                                             @Kwesidei not the whole crew   \n",
       "...                                                                                                                     ...  \n",
       "1599994                                                            Just woke up. Having no school is the best feeling ever   \n",
       "1599995                                      TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta  \n",
       "1599996                                                           Are you ready for your MoJo Makeover? Ask me for details   \n",
       "1599997                                                   Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur   \n",
       "1599998                                                      happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H   \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dframe)\n",
    "display(dframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five rows of the database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date      flag   \n",
       "0          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  \\\n",
       "1          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user   \n",
       "0  scotthamilton  \\\n",
       "1       mattycus   \n",
       "2        ElleCTF   \n",
       "3         Karoli   \n",
       "4       joy_wolf   \n",
       "\n",
       "                                                                                                              text  \n",
       "0  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "1                        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "2                                                                  my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   \n",
       "4                                                                                    @Kwesidei not the whole crew   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   sentiment  1599999 non-null  int64 \n",
      " 1   id         1599999 non-null  int64 \n",
      " 2   date       1599999 non-null  object\n",
      " 3   flag       1599999 non-null  object\n",
      " 4   user       1599999 non-null  object\n",
      " 5   text       1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.599999e+06</td>\n",
       "      <td>1.599999e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935757e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467811e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sentiment            id\n",
       "count  1.599999e+06  1.599999e+06\n",
       "mean   2.000001e+00  1.998818e+09\n",
       "std    2.000001e+00  1.935757e+08\n",
       "min    0.000000e+00  1.467811e+09\n",
       "25%    0.000000e+00  1.956916e+09\n",
       "50%    4.000000e+00  2.002102e+09\n",
       "75%    4.000000e+00  2.177059e+09\n",
       "max    4.000000e+00  2.329206e+09"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe.columns = const.columns\n",
    "print('The first five rows of the database')\n",
    "display(dframe.head())\n",
    "print('Information')\n",
    "display(dframe.info())\n",
    "print('Description')\n",
    "dframe.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a id=' Step 3.1.1'>Step 3.1.1.:</a> Investigate the Dataframe</h4>\n",
    "\n",
    "Now let us get the number of unique values in each column; we had \n",
    "\n",
    "**dframe.unstack().groupby(level=0).nunique()**\n",
    "\n",
    "as a suggestion but it was over 6 times slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment          2\n",
       "id           1598314\n",
       "date          774362\n",
       "flag               1\n",
       "user          659775\n",
       "text         1581465\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we had \n",
    "# dframe.unstack().groupby(level=0).nunique()\n",
    "# as a suggestion but it was over 6 times slower\n",
    "dframe.apply(pkg.pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentiment'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "4    800000\n",
       "0    799999\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'id'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "id\n",
       "2190457769    2\n",
       "1974742852    2\n",
       "2062516845    2\n",
       "1551586713    2\n",
       "1563681287    2\n",
       "             ..\n",
       "2197311343    1\n",
       "2197311196    1\n",
       "2197311146    1\n",
       "2197310899    1\n",
       "2193602129    1\n",
       "Name: count, Length: 1598314, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'date'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "date\n",
       "Mon Jun 15 12:53:14 PDT 2009    20\n",
       "Fri May 29 13:40:04 PDT 2009    17\n",
       "Mon Jun 15 13:39:50 PDT 2009    17\n",
       "Fri May 22 05:10:17 PDT 2009    17\n",
       "Fri Jun 05 11:05:33 PDT 2009    16\n",
       "                                ..\n",
       "Sun Jun 07 12:36:09 PDT 2009     1\n",
       "Sun Jun 07 12:36:07 PDT 2009     1\n",
       "Sun Jun 07 12:36:04 PDT 2009     1\n",
       "Sun Jun 07 12:36:03 PDT 2009     1\n",
       "Tue Jun 16 08:40:50 PDT 2009     1\n",
       "Name: count, Length: 774362, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'flag'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "flag\n",
       "NO_QUERY    1599999\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'user'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "user\n",
       "lost_dog           549\n",
       "webwoke            345\n",
       "tweetpet           310\n",
       "SallytheShizzle    281\n",
       "VioletsCRUK        279\n",
       "                  ... \n",
       "iheartrobpattz       1\n",
       "67trinity            1\n",
       "Sibby                1\n",
       "mAnyA_15             1\n",
       "bpbabe               1\n",
       "Name: count, Length: 659775, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text\n",
       "isPlayer Has Died! Sorry                                                                              210\n",
       "good morning                                                                                          118\n",
       "headache                                                                                              115\n",
       "Good morning                                                                                          112\n",
       "Headache                                                                                              106\n",
       "                                                                                                     ... \n",
       "braces  tell me it will be okay...                                                                      1\n",
       "is stuck at home without curry                                                                          1\n",
       "@mrsduryee I've applied to about 70 since I lost my job in March...it certainly FEELS like a lot!       1\n",
       "The cheese I got @SarawithanR lost its squeak                                                           1\n",
       "happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                                           1\n",
       "Name: count, Length: 1581465, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in dframe.columns:\n",
    "    display(_)\n",
    "    display(dframe[_].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To see the whole width of the table we make a small permanent change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg.pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if some of the functions work properly and how we can use them to gain more information about this database. Here we list all the tweets that are generated by the user 'lost_dog'. We choose to only print the texts and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43934              @NyleW I am lost. Please help me find a good home. \n",
      "45573             @SallyD I am lost. Please help me find a good home. \n",
      "46918         @zuppaholic I am lost. Please help me find a good home. \n",
      "47948         @LOSTPETUSA I am lost. Please help me find a good home. \n",
      "50571     @JeanLevertHood I am lost. Please help me find a good home. \n",
      "                                      ...                             \n",
      "792408       @trooppetrie I am lost. Please help me find a good home. \n",
      "793313         @Carly_FTS I am lost. Please help me find a good home. \n",
      "793609         @inathlone I am lost. Please help me find a good home. \n",
      "798607              @Kram I am lost. Please help me find a good home. \n",
      "799404         @W_Hancock I am lost. Please help me find a good home. \n",
      "Name: text, Length: 549, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dframe[dframe['user'].astype('string') == 'lost_dog']['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 50 users that generated the most tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lost_dog</th>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webwoke</th>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetpet</th>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SallytheShizzle</th>\n",
       "      <td>281</td>\n",
       "      <td>281</td>\n",
       "      <td>281</td>\n",
       "      <td>281</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VioletsCRUK</th>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcraddictal</th>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsarnick</th>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what_bugs_u</th>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karen230683</th>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DarkPiano</th>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SongoftheOss</th>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayme1988</th>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keza34</th>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ramdomthoughts</th>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shanajaca</th>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wowlew</th>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuttychris</th>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TraceyHewins</th>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thisgoeshere</th>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spidersamm</th>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StDAY</th>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>felicityfuller</th>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dogbook</th>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_magic8ball</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torilovesbradie</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutchrudder</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Djalfy</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twebbstack</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quimo</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enamoredsoul</th>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Broooooke_</th>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MTVnHollyWEST23</th>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JessMcFlyxxx</th>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiDesfileNegro</th>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linnetwoods</th>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KevinEdwardsJr</th>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insearchofnkotb</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karinb_za</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scyranth</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JBnVFCLover786</th>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cookiemonster82</th>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shellrawlins</th>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maynaseric</th>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hollyalyxfinch</th>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrs_mcsupergirl</th>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lesley007</th>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DonniesGirl69</th>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul_steele</th>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judez_xo</th>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patriciaco</th>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sentiment   id  date  flag  text\n",
       "user                                             \n",
       "lost_dog               549  549   549   549   549\n",
       "webwoke                345  345   345   345   345\n",
       "tweetpet               310  310   310   310   310\n",
       "SallytheShizzle        281  281   281   281   281\n",
       "VioletsCRUK            279  279   279   279   279\n",
       "mcraddictal            276  276   276   276   276\n",
       "tsarnick               248  248   248   248   248\n",
       "what_bugs_u            246  246   246   246   246\n",
       "Karen230683            238  238   238   238   238\n",
       "DarkPiano              236  236   236   236   236\n",
       "SongoftheOss           227  227   227   227   227\n",
       "Jayme1988              225  225   225   225   225\n",
       "keza34                 219  219   219   219   219\n",
       "ramdomthoughts         216  216   216   216   216\n",
       "shanajaca              213  213   213   213   213\n",
       "wowlew                 212  212   212   212   212\n",
       "nuttychris             211  211   211   211   211\n",
       "TraceyHewins           211  211   211   211   211\n",
       "thisgoeshere           207  207   207   207   207\n",
       "Spidersamm             205  205   205   205   205\n",
       "StDAY                  202  202   202   202   202\n",
       "felicityfuller         195  195   195   195   195\n",
       "Dogbook                192  192   192   192   192\n",
       "_magic8ball            189  189   189   189   189\n",
       "torilovesbradie        182  182   182   182   182\n",
       "Dutchrudder            182  182   182   182   182\n",
       "Djalfy                 182  182   182   182   182\n",
       "twebbstack             180  180   180   180   180\n",
       "Quimo                  180  180   180   180   180\n",
       "enamoredsoul           179  179   179   179   179\n",
       "Broooooke_             179  179   179   179   179\n",
       "MTVnHollyWEST23        178  178   178   178   178\n",
       "JessMcFlyxxx           178  178   178   178   178\n",
       "MiDesfileNegro         177  177   177   177   177\n",
       "linnetwoods            171  171   171   171   171\n",
       "KevinEdwardsJr         171  171   171   171   171\n",
       "insearchofnkotb        170  170   170   170   170\n",
       "karinb_za              166  166   166   166   166\n",
       "Scyranth               166  166   166   166   166\n",
       "JBnVFCLover786         163  163   163   163   163\n",
       "cookiemonster82        160  160   160   160   160\n",
       "shellrawlins           159  159   159   159   159\n",
       "maynaseric             159  159   159   159   159\n",
       "hollyalyxfinch         159  159   159   159   159\n",
       "mrs_mcsupergirl        158  158   158   158   158\n",
       "lesley007              158  158   158   158   158\n",
       "DonniesGirl69          155  155   155   155   155\n",
       "paul_steele            152  152   152   152   152\n",
       "judez_xo               152  152   152   152   152\n",
       "patriciaco             151  151   151   151   151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped_dframe = dframe.groupby(['user']).count()\n",
    "sorted_dframe = grouped_dframe.sort_values(by=['user'], ascending=False)\n",
    "largest_dframe = sorted_dframe.nlargest(50, 'flag')\n",
    "display(largest_dframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  span {\n",
    "    background-color: transparent;\n",
    "    color: orange;\n",
    "  }\n",
    "</style>\n",
    "What differs the tweets that share the same ID but are still saved in this dataframe, from the others, is that they have different sentiments; don't ask me why but some seem totally unreasonable. For instance, why should the following tweet, with the ID# 1467863684, be both positive and negative at the same time? Is it because they have mentioned the word \"sad\"?\n",
    "\n",
    "<span>Awwh babs... you look so sad underneith that shop entrance of &quot;Yesterday's Musik&quot; O-: I like the look of the new transformer movie</span>\n",
    "\n",
    "<h4><a id=' Step 3.1.2'>Step 3.1.2.:</a> Investigate the Sentiments</h4>\n",
    "\n",
    "As we see in [Investigating the Dataframe](#investigatingthedataframe) above, there are tweet IDs that are redundant. As a matter of fact, 1,598,314 out of 1,599,999 tweets have unique IDs. So, let us have a look at it and see what we can find out.\n",
    "\n",
    "<h5><a id=' Step 3.1.2.1'>Step 3.1.2.1.:</a> Find Duplicates</h5>\n",
    "\n",
    "Try and find how many tweets have more than one sentiment related to them. For this we count the occurrences of each value in the 'id' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "2190457769    2\n",
       "1974742852    2\n",
       "2062516845    2\n",
       "1551586713    2\n",
       "1563681287    2\n",
       "             ..\n",
       "2197311343    1\n",
       "2197311196    1\n",
       "2197311146    1\n",
       "2197310899    1\n",
       "2193602129    1\n",
       "Name: count, Length: 1598314, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_dframe_ids = dframe['id'].value_counts()\n",
    "display(all_dframe_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filter the result to include only values that appear twice or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "2190457769    2\n",
       "1974742852    2\n",
       "2062516845    2\n",
       "1551586713    2\n",
       "1563681287    2\n",
       "             ..\n",
       "2015412220    2\n",
       "2006617256    2\n",
       "1933201064    2\n",
       "2189722020    2\n",
       "1982279593    2\n",
       "Name: count, Length: 1685, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "duplicated_dframe_ids = all_dframe_ids[all_dframe_ids >= 2]\n",
    "display(duplicated_dframe_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows clearly that as we expected 1,599,999 - 1,598,314 = 1,685 tweets have double sentiments. So we print the rows in dframe for which the 'id' column is in repeated_values.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "      <td>1467863684</td>\n",
       "      <td>Mon Apr 06 22:33:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DjGundam</td>\n",
       "      <td>Awwh babs... you look so sad underneith that shop entrance of &amp;quot;Yesterday's Musik&amp;quot;  O-: I like the look of the new transformer movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800260</th>\n",
       "      <td>4</td>\n",
       "      <td>1467863684</td>\n",
       "      <td>Mon Apr 06 22:33:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DjGundam</td>\n",
       "      <td>Awwh babs... you look so sad underneith that shop entrance of &amp;quot;Yesterday's Musik&amp;quot;  O-: I like the look of the new transformer movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>1467880442</td>\n",
       "      <td>Mon Apr 06 22:38:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>iCalvin</td>\n",
       "      <td>Haven't tweeted nearly all day  Posted my website tonight, hopefully that goes well  Night time!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800299</th>\n",
       "      <td>4</td>\n",
       "      <td>1467880442</td>\n",
       "      <td>Mon Apr 06 22:38:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>iCalvin</td>\n",
       "      <td>Haven't tweeted nearly all day  Posted my website tonight, hopefully that goes well  Night time!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0</td>\n",
       "      <td>1468053611</td>\n",
       "      <td>Mon Apr 06 23:28:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mariejamora</td>\n",
       "      <td>@hellobebe I also send some updates in plurk but i upload photos on twitter!  you didnt see any of my updates on plurk? Zero?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801279</th>\n",
       "      <td>4</td>\n",
       "      <td>1468053611</td>\n",
       "      <td>Mon Apr 06 23:28:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mariejamora</td>\n",
       "      <td>@hellobebe I also send some updates in plurk but i upload photos on twitter!  you didnt see any of my updates on plurk? Zero?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>0</td>\n",
       "      <td>1468100580</td>\n",
       "      <td>Mon Apr 06 23:42:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cristygarza</td>\n",
       "      <td>good night swetdreamss to everyonee   and jared never chat in kyte puff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801572</th>\n",
       "      <td>4</td>\n",
       "      <td>1468100580</td>\n",
       "      <td>Mon Apr 06 23:42:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cristygarza</td>\n",
       "      <td>good night swetdreamss to everyonee   and jared never chat in kyte puff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>0</td>\n",
       "      <td>1468115720</td>\n",
       "      <td>Mon Apr 06 23:48:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>WarholGirl</td>\n",
       "      <td>@ientje89 aw i'm fine too thanks! yeah i miss you so much on the MFC  but hope we can talk later on today  kisses :huglove:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801649</th>\n",
       "      <td>4</td>\n",
       "      <td>1468115720</td>\n",
       "      <td>Mon Apr 06 23:48:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>WarholGirl</td>\n",
       "      <td>@ientje89 aw i'm fine too thanks! yeah i miss you so much on the MFC  but hope we can talk later on today  kisses :huglove:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>0</td>\n",
       "      <td>1468131748</td>\n",
       "      <td>Mon Apr 06 23:53:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>steveslee</td>\n",
       "      <td>@gordonchiu You're one letter alway!    Koreans don't use &amp;quot;X&amp;quot; so there's no hope for me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801793</th>\n",
       "      <td>4</td>\n",
       "      <td>1468131748</td>\n",
       "      <td>Mon Apr 06 23:53:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>steveslee</td>\n",
       "      <td>@gordonchiu You're one letter alway!    Koreans don't use &amp;quot;X&amp;quot; so there's no hope for me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>0</td>\n",
       "      <td>1468161883</td>\n",
       "      <td>Tue Apr 07 00:03:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>warrenaissance</td>\n",
       "      <td>I found my MADDEN '08!  in '09  ...oh well, I say Old is New Again!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801960</th>\n",
       "      <td>4</td>\n",
       "      <td>1468161883</td>\n",
       "      <td>Tue Apr 07 00:03:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>warrenaissance</td>\n",
       "      <td>I found my MADDEN '08!  in '09  ...oh well, I say Old is New Again!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802396</th>\n",
       "      <td>4</td>\n",
       "      <td>1468224250</td>\n",
       "      <td>Tue Apr 07 00:23:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lindseykanno</td>\n",
       "      <td>@friendlypharm  too bad it's true, for the most part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>0</td>\n",
       "      <td>1468224250</td>\n",
       "      <td>Tue Apr 07 00:23:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lindseykanno</td>\n",
       "      <td>@friendlypharm  too bad it's true, for the most part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802974</th>\n",
       "      <td>4</td>\n",
       "      <td>1468310350</td>\n",
       "      <td>Tue Apr 07 00:53:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tashhhhhhh</td>\n",
       "      <td>@dougiemcfly morning  i'm really upset  my rabbit ran away last night  and the postman woke me up early  reply? ilu x.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>0</td>\n",
       "      <td>1468310350</td>\n",
       "      <td>Tue Apr 07 00:53:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tashhhhhhh</td>\n",
       "      <td>@dougiemcfly morning  i'm really upset  my rabbit ran away last night  and the postman woke me up early  reply? ilu x.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803175</th>\n",
       "      <td>4</td>\n",
       "      <td>1468338634</td>\n",
       "      <td>Tue Apr 07 01:03:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>phampton</td>\n",
       "      <td>3 days leave then Easter, no work for a week,  Except for the long list of DIY jobs to do at home,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>0</td>\n",
       "      <td>1468338634</td>\n",
       "      <td>Tue Apr 07 01:03:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>phampton</td>\n",
       "      <td>3 days leave then Easter, no work for a week,  Except for the long list of DIY jobs to do at home,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803300</th>\n",
       "      <td>4</td>\n",
       "      <td>1468363676</td>\n",
       "      <td>Tue Apr 07 01:12:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LiL_Lullue</td>\n",
       "      <td>yay no work todayyy   but working for the rest of the week  lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>0</td>\n",
       "      <td>1468363676</td>\n",
       "      <td>Tue Apr 07 01:12:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LiL_Lullue</td>\n",
       "      <td>yay no work todayyy   but working for the rest of the week  lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>0</td>\n",
       "      <td>1468502040</td>\n",
       "      <td>Tue Apr 07 02:03:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>McFLYBelgium</td>\n",
       "      <td>@dougiemcfly @tommcfly good morning guys, how are you all? You know, it's frustrating, I never get a reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804315</th>\n",
       "      <td>4</td>\n",
       "      <td>1468502040</td>\n",
       "      <td>Tue Apr 07 02:03:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>McFLYBelgium</td>\n",
       "      <td>@dougiemcfly @tommcfly good morning guys, how are you all? You know, it's frustrating, I never get a reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>0</td>\n",
       "      <td>1468503801</td>\n",
       "      <td>Tue Apr 07 02:04:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinknevertalks</td>\n",
       "      <td>@redtoffee Strawberry is the absolute best Angel Delight EVA!  I had chocolate once, but it was too sweet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804348</th>\n",
       "      <td>4</td>\n",
       "      <td>1468503801</td>\n",
       "      <td>Tue Apr 07 02:04:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinknevertalks</td>\n",
       "      <td>@redtoffee Strawberry is the absolute best Angel Delight EVA!  I had chocolate once, but it was too sweet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>0</td>\n",
       "      <td>1468544973</td>\n",
       "      <td>Tue Apr 07 02:19:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>rkhooks</td>\n",
       "      <td>@lejeff oh pants! I'm hanging out with the old folks back  in England   Defo b up 4 1 when I get back. tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804655</th>\n",
       "      <td>4</td>\n",
       "      <td>1468544973</td>\n",
       "      <td>Tue Apr 07 02:19:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>rkhooks</td>\n",
       "      <td>@lejeff oh pants! I'm hanging out with the old folks back  in England   Defo b up 4 1 when I get back. tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>0</td>\n",
       "      <td>1468586841</td>\n",
       "      <td>Tue Apr 07 02:34:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Micc_x</td>\n",
       "      <td>@bradiewebbstack sway sway tour in julyyyyy! exitedd muchh  follow me pleaseeee? i need more followerss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804928</th>\n",
       "      <td>4</td>\n",
       "      <td>1468586841</td>\n",
       "      <td>Tue Apr 07 02:34:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Micc_x</td>\n",
       "      <td>@bradiewebbstack sway sway tour in julyyyyy! exitedd muchh  follow me pleaseeee? i need more followerss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805254</th>\n",
       "      <td>4</td>\n",
       "      <td>1468639063</td>\n",
       "      <td>Tue Apr 07 02:53:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kenguest</td>\n",
       "      <td>it'd be great if some opensource luminary would record 'talk' files for #rockbox  the daleky voice is unimpressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>0</td>\n",
       "      <td>1468639063</td>\n",
       "      <td>Tue Apr 07 02:53:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kenguest</td>\n",
       "      <td>it'd be great if some opensource luminary would record 'talk' files for #rockbox  the daleky voice is unimpressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>0</td>\n",
       "      <td>1468652839</td>\n",
       "      <td>Tue Apr 07 02:58:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>revjesse</td>\n",
       "      <td>@ecaps1 bloody idiot!!   just shop him into some gay porn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805340</th>\n",
       "      <td>4</td>\n",
       "      <td>1468652839</td>\n",
       "      <td>Tue Apr 07 02:58:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>revjesse</td>\n",
       "      <td>@ecaps1 bloody idiot!!   just shop him into some gay porn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805805</th>\n",
       "      <td>4</td>\n",
       "      <td>1468714181</td>\n",
       "      <td>Tue Apr 07 03:19:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheKZA</td>\n",
       "      <td>@iCoopers You tease  But thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>0</td>\n",
       "      <td>1468714181</td>\n",
       "      <td>Tue Apr 07 03:19:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheKZA</td>\n",
       "      <td>@iCoopers You tease  But thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>0</td>\n",
       "      <td>1468758512</td>\n",
       "      <td>Tue Apr 07 03:34:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_CBearT_</td>\n",
       "      <td>Had the best night in Letterfrack!;) lol suffering now tho  thank goodness richard's makin pancakes!!! Onto Galway city later  x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806090</th>\n",
       "      <td>4</td>\n",
       "      <td>1468758512</td>\n",
       "      <td>Tue Apr 07 03:34:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_CBearT_</td>\n",
       "      <td>Had the best night in Letterfrack!;) lol suffering now tho  thank goodness richard's makin pancakes!!! Onto Galway city later  x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806506</th>\n",
       "      <td>4</td>\n",
       "      <td>1468833927</td>\n",
       "      <td>Tue Apr 07 03:58:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>swimmingfishy</td>\n",
       "      <td>Going to school and enjoying my last day as a 16 year old  but  too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>0</td>\n",
       "      <td>1468833927</td>\n",
       "      <td>Tue Apr 07 03:58:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>swimmingfishy</td>\n",
       "      <td>Going to school and enjoying my last day as a 16 year old  but  too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>0</td>\n",
       "      <td>1468992236</td>\n",
       "      <td>Tue Apr 07 04:44:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tuxetuxe</td>\n",
       "      <td>humm no support for remote cvs history in opengrok 0.7 ...  guest i have to wait for 0.8!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807441</th>\n",
       "      <td>4</td>\n",
       "      <td>1468992236</td>\n",
       "      <td>Tue Apr 07 04:44:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tuxetuxe</td>\n",
       "      <td>humm no support for remote cvs history in opengrok 0.7 ...  guest i have to wait for 0.8!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5184</th>\n",
       "      <td>0</td>\n",
       "      <td>1469011145</td>\n",
       "      <td>Tue Apr 07 04:49:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SorenLorensen</td>\n",
       "      <td>@hollyalyxfinch Oh, Holly!  Take no notice of these morons - we think you're wonderful and very talented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807542</th>\n",
       "      <td>4</td>\n",
       "      <td>1469011145</td>\n",
       "      <td>Tue Apr 07 04:49:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SorenLorensen</td>\n",
       "      <td>@hollyalyxfinch Oh, Holly!  Take no notice of these morons - we think you're wonderful and very talented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6024</th>\n",
       "      <td>0</td>\n",
       "      <td>1469267615</td>\n",
       "      <td>Tue Apr 07 05:49:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>davideggleton</td>\n",
       "      <td>Today is very cold, so cold I may have to start wearing my jeans again  yesterday was raining but I did get some good shots of ducks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808644</th>\n",
       "      <td>4</td>\n",
       "      <td>1469267615</td>\n",
       "      <td>Tue Apr 07 05:49:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>davideggleton</td>\n",
       "      <td>Today is very cold, so cold I may have to start wearing my jeans again  yesterday was raining but I did get some good shots of ducks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809638</th>\n",
       "      <td>4</td>\n",
       "      <td>1469531660</td>\n",
       "      <td>Tue Apr 07 06:39:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>appleaddicto</td>\n",
       "      <td>Company blocked Twitter today  oh well i still have it on mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>0</td>\n",
       "      <td>1469531660</td>\n",
       "      <td>Tue Apr 07 06:39:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>appleaddicto</td>\n",
       "      <td>Company blocked Twitter today  oh well i still have it on mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809948</th>\n",
       "      <td>4</td>\n",
       "      <td>1469618724</td>\n",
       "      <td>Tue Apr 07 06:55:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CoopieCoop27</td>\n",
       "      <td>is loving the sun  but is upset she cant make the picnic thurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>0</td>\n",
       "      <td>1469618724</td>\n",
       "      <td>Tue Apr 07 06:55:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CoopieCoop27</td>\n",
       "      <td>is loving the sun  but is upset she cant make the picnic thurs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          id                          date      flag   \n",
       "212             0  1467863684  Mon Apr 06 22:33:35 PDT 2009  NO_QUERY  \\\n",
       "800260          4  1467863684  Mon Apr 06 22:33:35 PDT 2009  NO_QUERY   \n",
       "274             0  1467880442  Mon Apr 06 22:38:04 PDT 2009  NO_QUERY   \n",
       "800299          4  1467880442  Mon Apr 06 22:38:04 PDT 2009  NO_QUERY   \n",
       "988             0  1468053611  Mon Apr 06 23:28:09 PDT 2009  NO_QUERY   \n",
       "801279          4  1468053611  Mon Apr 06 23:28:09 PDT 2009  NO_QUERY   \n",
       "1176            0  1468100580  Mon Apr 06 23:42:57 PDT 2009  NO_QUERY   \n",
       "801572          4  1468100580  Mon Apr 06 23:42:57 PDT 2009  NO_QUERY   \n",
       "1253            0  1468115720  Mon Apr 06 23:48:00 PDT 2009  NO_QUERY   \n",
       "801649          4  1468115720  Mon Apr 06 23:48:00 PDT 2009  NO_QUERY   \n",
       "1332            0  1468131748  Mon Apr 06 23:53:22 PDT 2009  NO_QUERY   \n",
       "801793          4  1468131748  Mon Apr 06 23:53:22 PDT 2009  NO_QUERY   \n",
       "1476            0  1468161883  Tue Apr 07 00:03:10 PDT 2009  NO_QUERY   \n",
       "801960          4  1468161883  Tue Apr 07 00:03:10 PDT 2009  NO_QUERY   \n",
       "802396          4  1468224250  Tue Apr 07 00:23:49 PDT 2009  NO_QUERY   \n",
       "1751            0  1468224250  Tue Apr 07 00:23:49 PDT 2009  NO_QUERY   \n",
       "802974          4  1468310350  Tue Apr 07 00:53:48 PDT 2009  NO_QUERY   \n",
       "2126            0  1468310350  Tue Apr 07 00:53:48 PDT 2009  NO_QUERY   \n",
       "803175          4  1468338634  Tue Apr 07 01:03:49 PDT 2009  NO_QUERY   \n",
       "2251            0  1468338634  Tue Apr 07 01:03:49 PDT 2009  NO_QUERY   \n",
       "803300          4  1468363676  Tue Apr 07 01:12:56 PDT 2009  NO_QUERY   \n",
       "2374            0  1468363676  Tue Apr 07 01:12:56 PDT 2009  NO_QUERY   \n",
       "3028            0  1468502040  Tue Apr 07 02:03:41 PDT 2009  NO_QUERY   \n",
       "804315          4  1468502040  Tue Apr 07 02:03:41 PDT 2009  NO_QUERY   \n",
       "3038            0  1468503801  Tue Apr 07 02:04:18 PDT 2009  NO_QUERY   \n",
       "804348          4  1468503801  Tue Apr 07 02:04:18 PDT 2009  NO_QUERY   \n",
       "3211            0  1468544973  Tue Apr 07 02:19:32 PDT 2009  NO_QUERY   \n",
       "804655          4  1468544973  Tue Apr 07 02:19:32 PDT 2009  NO_QUERY   \n",
       "3399            0  1468586841  Tue Apr 07 02:34:25 PDT 2009  NO_QUERY   \n",
       "804928          4  1468586841  Tue Apr 07 02:34:25 PDT 2009  NO_QUERY   \n",
       "805254          4  1468639063  Tue Apr 07 02:53:24 PDT 2009  NO_QUERY   \n",
       "3631            0  1468639063  Tue Apr 07 02:53:24 PDT 2009  NO_QUERY   \n",
       "3697            0  1468652839  Tue Apr 07 02:58:23 PDT 2009  NO_QUERY   \n",
       "805340          4  1468652839  Tue Apr 07 02:58:23 PDT 2009  NO_QUERY   \n",
       "805805          4  1468714181  Tue Apr 07 03:19:34 PDT 2009  NO_QUERY   \n",
       "3970            0  1468714181  Tue Apr 07 03:19:34 PDT 2009  NO_QUERY   \n",
       "4170            0  1468758512  Tue Apr 07 03:34:37 PDT 2009  NO_QUERY   \n",
       "806090          4  1468758512  Tue Apr 07 03:34:37 PDT 2009  NO_QUERY   \n",
       "806506          4  1468833927  Tue Apr 07 03:58:41 PDT 2009  NO_QUERY   \n",
       "4479            0  1468833927  Tue Apr 07 03:58:41 PDT 2009  NO_QUERY   \n",
       "5115            0  1468992236  Tue Apr 07 04:44:49 PDT 2009  NO_QUERY   \n",
       "807441          4  1468992236  Tue Apr 07 04:44:49 PDT 2009  NO_QUERY   \n",
       "5184            0  1469011145  Tue Apr 07 04:49:48 PDT 2009  NO_QUERY   \n",
       "807542          4  1469011145  Tue Apr 07 04:49:48 PDT 2009  NO_QUERY   \n",
       "6024            0  1469267615  Tue Apr 07 05:49:29 PDT 2009  NO_QUERY   \n",
       "808644          4  1469267615  Tue Apr 07 05:49:29 PDT 2009  NO_QUERY   \n",
       "809638          4  1469531660  Tue Apr 07 06:39:53 PDT 2009  NO_QUERY   \n",
       "6729            0  1469531660  Tue Apr 07 06:39:53 PDT 2009  NO_QUERY   \n",
       "809948          4  1469618724  Tue Apr 07 06:55:21 PDT 2009  NO_QUERY   \n",
       "6946            0  1469618724  Tue Apr 07 06:55:21 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user   \n",
       "212           DjGundam  \\\n",
       "800260        DjGundam   \n",
       "274            iCalvin   \n",
       "800299         iCalvin   \n",
       "988        mariejamora   \n",
       "801279     mariejamora   \n",
       "1176       cristygarza   \n",
       "801572     cristygarza   \n",
       "1253        WarholGirl   \n",
       "801649      WarholGirl   \n",
       "1332         steveslee   \n",
       "801793       steveslee   \n",
       "1476    warrenaissance   \n",
       "801960  warrenaissance   \n",
       "802396    lindseykanno   \n",
       "1751      lindseykanno   \n",
       "802974      tashhhhhhh   \n",
       "2126        tashhhhhhh   \n",
       "803175        phampton   \n",
       "2251          phampton   \n",
       "803300      LiL_Lullue   \n",
       "2374        LiL_Lullue   \n",
       "3028      McFLYBelgium   \n",
       "804315    McFLYBelgium   \n",
       "3038    tinknevertalks   \n",
       "804348  tinknevertalks   \n",
       "3211           rkhooks   \n",
       "804655         rkhooks   \n",
       "3399            Micc_x   \n",
       "804928          Micc_x   \n",
       "805254        kenguest   \n",
       "3631          kenguest   \n",
       "3697          revjesse   \n",
       "805340        revjesse   \n",
       "805805          TheKZA   \n",
       "3970            TheKZA   \n",
       "4170          _CBearT_   \n",
       "806090        _CBearT_   \n",
       "806506   swimmingfishy   \n",
       "4479     swimmingfishy   \n",
       "5115          tuxetuxe   \n",
       "807441        tuxetuxe   \n",
       "5184     SorenLorensen   \n",
       "807542   SorenLorensen   \n",
       "6024     davideggleton   \n",
       "808644   davideggleton   \n",
       "809638    appleaddicto   \n",
       "6729      appleaddicto   \n",
       "809948    CoopieCoop27   \n",
       "6946      CoopieCoop27   \n",
       "\n",
       "                                                                                                                                                  text  \n",
       "212     Awwh babs... you look so sad underneith that shop entrance of &quot;Yesterday's Musik&quot;  O-: I like the look of the new transformer movie   \n",
       "800260  Awwh babs... you look so sad underneith that shop entrance of &quot;Yesterday's Musik&quot;  O-: I like the look of the new transformer movie   \n",
       "274                                                   Haven't tweeted nearly all day  Posted my website tonight, hopefully that goes well  Night time!  \n",
       "800299                                                Haven't tweeted nearly all day  Posted my website tonight, hopefully that goes well  Night time!  \n",
       "988                     @hellobebe I also send some updates in plurk but i upload photos on twitter!  you didnt see any of my updates on plurk? Zero?   \n",
       "801279                  @hellobebe I also send some updates in plurk but i upload photos on twitter!  you didnt see any of my updates on plurk? Zero?   \n",
       "1176                                                                          good night swetdreamss to everyonee   and jared never chat in kyte puff   \n",
       "801572                                                                        good night swetdreamss to everyonee   and jared never chat in kyte puff   \n",
       "1253                       @ientje89 aw i'm fine too thanks! yeah i miss you so much on the MFC  but hope we can talk later on today  kisses :huglove:  \n",
       "801649                     @ientje89 aw i'm fine too thanks! yeah i miss you so much on the MFC  but hope we can talk later on today  kisses :huglove:  \n",
       "1332                                               @gordonchiu You're one letter alway!    Koreans don't use &quot;X&quot; so there's no hope for me.   \n",
       "801793                                             @gordonchiu You're one letter alway!    Koreans don't use &quot;X&quot; so there's no hope for me.   \n",
       "1476                                                                               I found my MADDEN '08!  in '09  ...oh well, I say Old is New Again!  \n",
       "801960                                                                             I found my MADDEN '08!  in '09  ...oh well, I say Old is New Again!  \n",
       "802396                                                                                           @friendlypharm  too bad it's true, for the most part   \n",
       "1751                                                                                             @friendlypharm  too bad it's true, for the most part   \n",
       "802974                          @dougiemcfly morning  i'm really upset  my rabbit ran away last night  and the postman woke me up early  reply? ilu x.  \n",
       "2126                            @dougiemcfly morning  i'm really upset  my rabbit ran away last night  and the postman woke me up early  reply? ilu x.  \n",
       "803175                                             3 days leave then Easter, no work for a week,  Except for the long list of DIY jobs to do at home,   \n",
       "2251                                               3 days leave then Easter, no work for a week,  Except for the long list of DIY jobs to do at home,   \n",
       "803300                                                                                 yay no work todayyy   but working for the rest of the week  lol  \n",
       "2374                                                                                   yay no work todayyy   but working for the rest of the week  lol  \n",
       "3028                                      @dougiemcfly @tommcfly good morning guys, how are you all? You know, it's frustrating, I never get a reply    \n",
       "804315                                    @dougiemcfly @tommcfly good morning guys, how are you all? You know, it's frustrating, I never get a reply    \n",
       "3038                                       @redtoffee Strawberry is the absolute best Angel Delight EVA!  I had chocolate once, but it was too sweet.   \n",
       "804348                                     @redtoffee Strawberry is the absolute best Angel Delight EVA!  I had chocolate once, but it was too sweet.   \n",
       "3211                                       @lejeff oh pants! I'm hanging out with the old folks back  in England   Defo b up 4 1 when I get back. tho   \n",
       "804655                                     @lejeff oh pants! I'm hanging out with the old folks back  in England   Defo b up 4 1 when I get back. tho   \n",
       "3399                                          @bradiewebbstack sway sway tour in julyyyyy! exitedd muchh  follow me pleaseeee? i need more followerss   \n",
       "804928                                        @bradiewebbstack sway sway tour in julyyyyy! exitedd muchh  follow me pleaseeee? i need more followerss   \n",
       "805254                             it'd be great if some opensource luminary would record 'talk' files for #rockbox  the daleky voice is unimpressive   \n",
       "3631                               it'd be great if some opensource luminary would record 'talk' files for #rockbox  the daleky voice is unimpressive   \n",
       "3697                                                                                        @ecaps1 bloody idiot!!   just shop him into some gay porn   \n",
       "805340                                                                                      @ecaps1 bloody idiot!!   just shop him into some gay porn   \n",
       "805805                                                                                                             @iCoopers You tease  But thank you   \n",
       "3970                                                                                                               @iCoopers You tease  But thank you   \n",
       "4170                  Had the best night in Letterfrack!;) lol suffering now tho  thank goodness richard's makin pancakes!!! Onto Galway city later  x  \n",
       "806090                Had the best night in Letterfrack!;) lol suffering now tho  thank goodness richard's makin pancakes!!! Onto Galway city later  x  \n",
       "806506                                                                             Going to school and enjoying my last day as a 16 year old  but  too  \n",
       "4479                                                                               Going to school and enjoying my last day as a 16 year old  but  too  \n",
       "5115                                                       humm no support for remote cvs history in opengrok 0.7 ...  guest i have to wait for 0.8!!   \n",
       "807441                                                     humm no support for remote cvs history in opengrok 0.7 ...  guest i have to wait for 0.8!!   \n",
       "5184                                         @hollyalyxfinch Oh, Holly!  Take no notice of these morons - we think you're wonderful and very talented   \n",
       "807542                                       @hollyalyxfinch Oh, Holly!  Take no notice of these morons - we think you're wonderful and very talented   \n",
       "6024             Today is very cold, so cold I may have to start wearing my jeans again  yesterday was raining but I did get some good shots of ducks   \n",
       "808644           Today is very cold, so cold I may have to start wearing my jeans again  yesterday was raining but I did get some good shots of ducks   \n",
       "809638                                                                               Company blocked Twitter today  oh well i still have it on mobile   \n",
       "6729                                                                                 Company blocked Twitter today  oh well i still have it on mobile   \n",
       "809948                                                                                 is loving the sun  but is upset she cant make the picnic thurs   \n",
       "6946                                                                                   is loving the sun  but is upset she cant make the picnic thurs   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "duplicated_tweet_dframe = dframe[dframe['id'].isin(duplicated_dframe_ids.index)]\n",
    "display(duplicated_tweet_dframe.sort_values(by='id').head(50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if those rows with duplicate ids have one 0 and one 4 in the sentiment column. We do it by adding the sentiments for each duplicated tweet. If they all have 4 as the result of the aggregation, we can conclude that each duplicate has received one 0 and one 4 as sentiment value. Consequenlty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1467863684</th>\n",
       "      <td>4</td>\n",
       "      <td>Awwh babs... you look so sad underneith that shop entrance of &amp;quot;Yesterday's Musik&amp;quot;  O-: I like the look of the new transformer movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467880442</th>\n",
       "      <td>4</td>\n",
       "      <td>Haven't tweeted nearly all day  Posted my website tonight, hopefully that goes well  Night time!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468053611</th>\n",
       "      <td>4</td>\n",
       "      <td>@hellobebe I also send some updates in plurk but i upload photos on twitter!  you didnt see any of my updates on plurk? Zero?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468100580</th>\n",
       "      <td>4</td>\n",
       "      <td>good night swetdreamss to everyonee   and jared never chat in kyte puff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468115720</th>\n",
       "      <td>4</td>\n",
       "      <td>@ientje89 aw i'm fine too thanks! yeah i miss you so much on the MFC  but hope we can talk later on today  kisses :huglove:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193278017</th>\n",
       "      <td>4</td>\n",
       "      <td>oh dear HH is back   please twitter do something about her.  I'm begging you, please pretty please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193403830</th>\n",
       "      <td>4</td>\n",
       "      <td>english exam went okay        revising for french, r.e and geography now, urrff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193428118</th>\n",
       "      <td>4</td>\n",
       "      <td>finally finished typing!!!! Woohoooo  , still need to add graphs though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193451289</th>\n",
       "      <td>4</td>\n",
       "      <td>@fanafatin see, @misschimichanga tweet u to join us!! u really cant?  so if thurs, when &amp;amp; where?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193576442</th>\n",
       "      <td>4</td>\n",
       "      <td>Had an injection today. Not fun  the rrst of the school day has been good. Tonight i am going dancing yey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1685 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentiment   \n",
       "id                      \n",
       "1467863684          4  \\\n",
       "1467880442          4   \n",
       "1468053611          4   \n",
       "1468100580          4   \n",
       "1468115720          4   \n",
       "...               ...   \n",
       "2193278017          4   \n",
       "2193403830          4   \n",
       "2193428118          4   \n",
       "2193451289          4   \n",
       "2193576442          4   \n",
       "\n",
       "                                                                                                                                                      text  \n",
       "id                                                                                                                                                          \n",
       "1467863684  Awwh babs... you look so sad underneith that shop entrance of &quot;Yesterday's Musik&quot;  O-: I like the look of the new transformer movie   \n",
       "1467880442                                                Haven't tweeted nearly all day  Posted my website tonight, hopefully that goes well  Night time!  \n",
       "1468053611                  @hellobebe I also send some updates in plurk but i upload photos on twitter!  you didnt see any of my updates on plurk? Zero?   \n",
       "1468100580                                                                        good night swetdreamss to everyonee   and jared never chat in kyte puff   \n",
       "1468115720                     @ientje89 aw i'm fine too thanks! yeah i miss you so much on the MFC  but hope we can talk later on today  kisses :huglove:  \n",
       "...                                                                                                                                                    ...  \n",
       "2193278017                                             oh dear HH is back   please twitter do something about her.  I'm begging you, please pretty please   \n",
       "2193403830                                                                english exam went okay        revising for french, r.e and geography now, urrff   \n",
       "2193428118                                                                        finally finished typing!!!! Woohoooo  , still need to add graphs though   \n",
       "2193451289                                           @fanafatin see, @misschimichanga tweet u to join us!! u really cant?  so if thurs, when &amp; where?   \n",
       "2193576442                                      Had an injection today. Not fun  the rrst of the school day has been good. Tonight i am going dancing yey   \n",
       "\n",
       "[1685 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aggregated_sentiments = duplicated_tweet_dframe.groupby('id').agg({'sentiment': 'sum', 'text': 'first'})\n",
    "display(aggregated_sentiments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be tested by asking if the number of 4s in the 'sentiment' column is the same as the size of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "4    1685\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_sentiments['sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this means <h3>YES</h3> We can go on with the next step and try to remove one of the duplicates and change the sentiment of the other one to 2, which is the same as 'natural'\n",
    "\n",
    "<h5><a id=' Step 3.1.2.2'>Step 3.1.2.2.:</a> Remove Duplicates</h5>\n",
    "\n",
    "Let us remove those rows in dframe that have the same id as in duplicated_dframe_ids and a sentiment of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1598314 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date      flag   \n",
       "0                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  \\\n",
       "1                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599994          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user   \n",
       "0          scotthamilton  \\\n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "                                                                                                                    text  \n",
       "0        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "1                              @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "2                                                                        my whole body feels itchy and like its on fire   \n",
       "3        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   \n",
       "4                                                                                          @Kwesidei not the whole crew   \n",
       "...                                                                                                                  ...  \n",
       "1599994                                                         Just woke up. Having no school is the best feeling ever   \n",
       "1599995                                   TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta  \n",
       "1599996                                                        Are you ready for your MoJo Makeover? Ask me for details   \n",
       "1599997                                                Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur   \n",
       "1599998                                                   happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H   \n",
       "\n",
       "[1598314 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dframe_wihout_duplicates = dframe[~(dframe['id'].isin(duplicated_dframe_ids.index) & (dframe['sentiment'] == 0))]\n",
    "display(dframe_wihout_duplicates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><a id=' Step 3.1.2.3'>Step 3.1.2.3.:</a> Neutralize the Sentiments</h5>\n",
    "\n",
    "Set the sentiment of those rows in 'dframe' that are mentioned in the list of duplicated ids 'duplicated_dframe_ids' to 2 to mean that they should be classified - or percepted - as neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1598314 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date      flag   \n",
       "0                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  \\\n",
       "1                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599994          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user   \n",
       "0          scotthamilton  \\\n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "                                                                                                                    text  \n",
       "0        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "1                              @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "2                                                                        my whole body feels itchy and like its on fire   \n",
       "3        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   \n",
       "4                                                                                          @Kwesidei not the whole crew   \n",
       "...                                                                                                                  ...  \n",
       "1599994                                                         Just woke up. Having no school is the best feeling ever   \n",
       "1599995                                   TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta  \n",
       "1599996                                                        Are you ready for your MoJo Makeover? Ask me for details   \n",
       "1599997                                                Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur   \n",
       "1599998                                                   happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H   \n",
       "\n",
       "[1598314 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change the sentiment of those rows in dframe_wihout_duplicates that are listed in duplicated_dframe_ids to 2\n",
    "neutralized_dframe = dframe_wihout_duplicates.copy()\n",
    "neutralized_dframe.loc[neutralized_dframe['id'].isin(duplicated_dframe_ids.index), 'sentiment'] = 2\n",
    "display(neutralized_dframe)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if the change has worked out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutralized_dframe.apply(pkg.pd.Series.nunique)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes indeed; we have three categories of sentiment now. And to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutralized_dframe['sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a id=' Step 3.1.3'>Step 3.1.3.:</a> Clean Out</h4>\n",
    "\n",
    "We also skip the flag, that has no function in any types of investigations. Perhaps if we would combine it with other databases, we'll have to take it back. But for now, we just drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagless_dframe = neutralized_dframe.drop(columns=['flag'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812416</td>\n",
       "      <td>Mon Apr 06 22:20:16 PDT 2009</td>\n",
       "      <td>erinx3leannexo</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812579</td>\n",
       "      <td>Mon Apr 06 22:20:17 PDT 2009</td>\n",
       "      <td>pardonlauren</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812723</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>TLeC</td>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812771</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>robrobbierobert</td>\n",
       "      <td>@octolinz16 It it counts, idk why I did either. you never talk to me anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812784</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>bayofwolves</td>\n",
       "      <td>@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812799</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>HairByJess</td>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812964</td>\n",
       "      <td>Mon Apr 06 22:20:22 PDT 2009</td>\n",
       "      <td>lovesongwriter</td>\n",
       "      <td>Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813137</td>\n",
       "      <td>Mon Apr 06 22:20:25 PDT 2009</td>\n",
       "      <td>armotley</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813579</td>\n",
       "      <td>Mon Apr 06 22:20:31 PDT 2009</td>\n",
       "      <td>starkissed</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  love the soundtrack!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813782</td>\n",
       "      <td>Mon Apr 06 22:20:34 PDT 2009</td>\n",
       "      <td>gi_gi_bee</td>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813985</td>\n",
       "      <td>Mon Apr 06 22:20:37 PDT 2009</td>\n",
       "      <td>quanvu</td>\n",
       "      <td>@alydesigns i was out most of the day so didn't get much done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813992</td>\n",
       "      <td>Mon Apr 06 22:20:38 PDT 2009</td>\n",
       "      <td>swinspeedx</td>\n",
       "      <td>one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814119</td>\n",
       "      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n",
       "      <td>cooliodoc</td>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814180</td>\n",
       "      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n",
       "      <td>viJILLante</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814192</td>\n",
       "      <td>Mon Apr 06 22:20:41 PDT 2009</td>\n",
       "      <td>Ljelli3166</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814438</td>\n",
       "      <td>Mon Apr 06 22:20:44 PDT 2009</td>\n",
       "      <td>ChicagoCubbie</td>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814783</td>\n",
       "      <td>Mon Apr 06 22:20:50 PDT 2009</td>\n",
       "      <td>KatieAngell</td>\n",
       "      <td>Just going to cry myself to sleep after watching Marley and Me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814883</td>\n",
       "      <td>Mon Apr 06 22:20:52 PDT 2009</td>\n",
       "      <td>gagoo</td>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815199</td>\n",
       "      <td>Mon Apr 06 22:20:56 PDT 2009</td>\n",
       "      <td>abel209</td>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815753</td>\n",
       "      <td>Mon Apr 06 22:21:04 PDT 2009</td>\n",
       "      <td>BaptisteTheFool</td>\n",
       "      <td>Meh... Almost Lover is the exception... this track gets me depressed every time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815923</td>\n",
       "      <td>Mon Apr 06 22:21:07 PDT 2009</td>\n",
       "      <td>fatkat309</td>\n",
       "      <td>some1 hacked my account on aim  now i have to make a new one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815924</td>\n",
       "      <td>Mon Apr 06 22:21:07 PDT 2009</td>\n",
       "      <td>EmCDL</td>\n",
       "      <td>@alielayus I want to go to promote GEAR AND GROOVE but unfornately no ride there  I may b going to the one in Anaheim in May though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815988</td>\n",
       "      <td>Mon Apr 06 22:21:09 PDT 2009</td>\n",
       "      <td>merisssa</td>\n",
       "      <td>thought sleeping in was an option tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>1467816149</td>\n",
       "      <td>Mon Apr 06 22:21:11 PDT 2009</td>\n",
       "      <td>Pbearfox</td>\n",
       "      <td>@julieebaby awe i love you too!!!! 1 am here  i miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1467816665</td>\n",
       "      <td>Mon Apr 06 22:21:21 PDT 2009</td>\n",
       "      <td>jsoo</td>\n",
       "      <td>@HumpNinja I cry my asian eyes to sleep at night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>1467816749</td>\n",
       "      <td>Mon Apr 06 22:21:20 PDT 2009</td>\n",
       "      <td>scarletletterm</td>\n",
       "      <td>ok I'm sick and spent an hour sitting in the shower cause I was too sick to stand and held back the puke like a champ. BED now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1467817225</td>\n",
       "      <td>Mon Apr 06 22:21:27 PDT 2009</td>\n",
       "      <td>crosland_12</td>\n",
       "      <td>@cocomix04 ill tell ya the story later  not a good day and ill be workin for like three more hours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>1467817374</td>\n",
       "      <td>Mon Apr 06 22:21:30 PDT 2009</td>\n",
       "      <td>ajaxpro</td>\n",
       "      <td>@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>1467817502</td>\n",
       "      <td>Mon Apr 06 22:21:32 PDT 2009</td>\n",
       "      <td>Tmttq86</td>\n",
       "      <td>@fleurylis I don't either. Its depressing. I don't think I even want to know about the kids in suitcases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>1467818007</td>\n",
       "      <td>Mon Apr 06 22:21:39 PDT 2009</td>\n",
       "      <td>Anthony_Nguyen</td>\n",
       "      <td>Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>1467818020</td>\n",
       "      <td>Mon Apr 06 22:21:39 PDT 2009</td>\n",
       "      <td>itsanimesh</td>\n",
       "      <td>really don't feel like getting up today... but got to study to for tomorrows practical exam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>1467818481</td>\n",
       "      <td>Mon Apr 06 22:21:46 PDT 2009</td>\n",
       "      <td>lionslamb</td>\n",
       "      <td>He's the reason for the teardrops on my guitar the only one who has enough of me to break my heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>1467818603</td>\n",
       "      <td>Mon Apr 06 22:21:49 PDT 2009</td>\n",
       "      <td>kennypham</td>\n",
       "      <td>Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>1467818900</td>\n",
       "      <td>Mon Apr 06 22:21:53 PDT 2009</td>\n",
       "      <td>DdubsShellBell</td>\n",
       "      <td>@JonathanRKnight Awww I soo wish I was there to see you finally comfortable! Im sad that I missed it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>1467819022</td>\n",
       "      <td>Mon Apr 06 22:21:56 PDT 2009</td>\n",
       "      <td>hpfangirl94</td>\n",
       "      <td>Falling asleep. Just heard about that Tracy girl's body being found. How sad  My heart breaks for that family.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>1467819650</td>\n",
       "      <td>Mon Apr 06 22:22:05 PDT 2009</td>\n",
       "      <td>antzpantz</td>\n",
       "      <td>@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>1467819712</td>\n",
       "      <td>Mon Apr 06 22:22:06 PDT 2009</td>\n",
       "      <td>labrt2004</td>\n",
       "      <td>Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>1467819812</td>\n",
       "      <td>Mon Apr 06 22:22:07 PDT 2009</td>\n",
       "      <td>IrisJumbe</td>\n",
       "      <td>Oh man...was ironing @jeancjumbe's fave top to wear to a meeting. Burnt it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>1467820206</td>\n",
       "      <td>Mon Apr 06 22:22:13 PDT 2009</td>\n",
       "      <td>peacoats</td>\n",
       "      <td>is strangely sad about LiLo and SamRo breaking up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>1467820835</td>\n",
       "      <td>Mon Apr 06 22:22:25 PDT 2009</td>\n",
       "      <td>cyantist</td>\n",
       "      <td>@tea oh! i'm so sorry  i didn't think about that before retweeting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>1467820863</td>\n",
       "      <td>Mon Apr 06 22:22:23 PDT 2009</td>\n",
       "      <td>tautao</td>\n",
       "      <td>Broadband plan 'a massive broken promise' http://tinyurl.com/dcuc33 via www.diigo.com/~tautao Still waiting for broadband we are</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment          id                          date             user   \n",
       "0           0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton  \\\n",
       "1           0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "2           0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "3           0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "4           0  1467811372  Mon Apr 06 22:20:00 PDT 2009         joy_wolf   \n",
       "5           0  1467811592  Mon Apr 06 22:20:03 PDT 2009          mybirch   \n",
       "6           0  1467811594  Mon Apr 06 22:20:03 PDT 2009             coZZ   \n",
       "7           0  1467811795  Mon Apr 06 22:20:05 PDT 2009  2Hood4Hollywood   \n",
       "8           0  1467812025  Mon Apr 06 22:20:09 PDT 2009          mimismo   \n",
       "9           0  1467812416  Mon Apr 06 22:20:16 PDT 2009   erinx3leannexo   \n",
       "10          0  1467812579  Mon Apr 06 22:20:17 PDT 2009     pardonlauren   \n",
       "11          0  1467812723  Mon Apr 06 22:20:19 PDT 2009             TLeC   \n",
       "12          0  1467812771  Mon Apr 06 22:20:19 PDT 2009  robrobbierobert   \n",
       "13          0  1467812784  Mon Apr 06 22:20:20 PDT 2009      bayofwolves   \n",
       "14          0  1467812799  Mon Apr 06 22:20:20 PDT 2009       HairByJess   \n",
       "15          0  1467812964  Mon Apr 06 22:20:22 PDT 2009   lovesongwriter   \n",
       "16          0  1467813137  Mon Apr 06 22:20:25 PDT 2009         armotley   \n",
       "17          0  1467813579  Mon Apr 06 22:20:31 PDT 2009       starkissed   \n",
       "18          0  1467813782  Mon Apr 06 22:20:34 PDT 2009        gi_gi_bee   \n",
       "19          0  1467813985  Mon Apr 06 22:20:37 PDT 2009           quanvu   \n",
       "20          0  1467813992  Mon Apr 06 22:20:38 PDT 2009       swinspeedx   \n",
       "21          0  1467814119  Mon Apr 06 22:20:40 PDT 2009        cooliodoc   \n",
       "22          0  1467814180  Mon Apr 06 22:20:40 PDT 2009       viJILLante   \n",
       "23          0  1467814192  Mon Apr 06 22:20:41 PDT 2009       Ljelli3166   \n",
       "24          0  1467814438  Mon Apr 06 22:20:44 PDT 2009    ChicagoCubbie   \n",
       "25          0  1467814783  Mon Apr 06 22:20:50 PDT 2009      KatieAngell   \n",
       "26          0  1467814883  Mon Apr 06 22:20:52 PDT 2009            gagoo   \n",
       "27          0  1467815199  Mon Apr 06 22:20:56 PDT 2009          abel209   \n",
       "28          0  1467815753  Mon Apr 06 22:21:04 PDT 2009  BaptisteTheFool   \n",
       "29          0  1467815923  Mon Apr 06 22:21:07 PDT 2009        fatkat309   \n",
       "30          0  1467815924  Mon Apr 06 22:21:07 PDT 2009            EmCDL   \n",
       "31          0  1467815988  Mon Apr 06 22:21:09 PDT 2009         merisssa   \n",
       "32          0  1467816149  Mon Apr 06 22:21:11 PDT 2009         Pbearfox   \n",
       "33          0  1467816665  Mon Apr 06 22:21:21 PDT 2009             jsoo   \n",
       "34          0  1467816749  Mon Apr 06 22:21:20 PDT 2009   scarletletterm   \n",
       "35          0  1467817225  Mon Apr 06 22:21:27 PDT 2009      crosland_12   \n",
       "36          0  1467817374  Mon Apr 06 22:21:30 PDT 2009          ajaxpro   \n",
       "37          0  1467817502  Mon Apr 06 22:21:32 PDT 2009          Tmttq86   \n",
       "38          0  1467818007  Mon Apr 06 22:21:39 PDT 2009   Anthony_Nguyen   \n",
       "39          0  1467818020  Mon Apr 06 22:21:39 PDT 2009       itsanimesh   \n",
       "40          0  1467818481  Mon Apr 06 22:21:46 PDT 2009        lionslamb   \n",
       "41          0  1467818603  Mon Apr 06 22:21:49 PDT 2009        kennypham   \n",
       "42          0  1467818900  Mon Apr 06 22:21:53 PDT 2009   DdubsShellBell   \n",
       "43          0  1467819022  Mon Apr 06 22:21:56 PDT 2009      hpfangirl94   \n",
       "44          0  1467819650  Mon Apr 06 22:22:05 PDT 2009        antzpantz   \n",
       "45          0  1467819712  Mon Apr 06 22:22:06 PDT 2009        labrt2004   \n",
       "46          0  1467819812  Mon Apr 06 22:22:07 PDT 2009        IrisJumbe   \n",
       "47          0  1467820206  Mon Apr 06 22:22:13 PDT 2009         peacoats   \n",
       "48          0  1467820835  Mon Apr 06 22:22:25 PDT 2009         cyantist   \n",
       "49          0  1467820863  Mon Apr 06 22:22:23 PDT 2009           tautao   \n",
       "\n",
       "                                                                                                                                          text  \n",
       "0                              is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "1                                                    @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "2                                                                                              my whole body feels itchy and like its on fire   \n",
       "3                              @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   \n",
       "4                                                                                                                @Kwesidei not the whole crew   \n",
       "5                                                                                                                                  Need a hug   \n",
       "6                                          @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?  \n",
       "7                                                                                                         @Tatiana_K nope they didn't have it   \n",
       "8                                                                                                                    @twittera que me muera ?   \n",
       "9                                                                                                  spring break in plain city... it's snowing   \n",
       "10                                                                                                                  I just re-pierced my ears   \n",
       "11                                              @caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .  \n",
       "12                                                               @octolinz16 It it counts, idk why I did either. you never talk to me anymore   \n",
       "13                       @smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.  \n",
       "14                                     @iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!  \n",
       "15                                               Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?  \n",
       "16                                                                                                                        about to file taxes   \n",
       "17                                                                            @LettyA ahh ive always wanted to see rent  love the soundtrack!!  \n",
       "18                                                             @FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?   \n",
       "19                                                                              @alydesigns i was out most of the day so didn't get much done   \n",
       "20                                       one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*   \n",
       "21                                                                                            @angry_barista I baked you a cake but I ated it   \n",
       "22                                                                                                      this week is not going as i had hoped   \n",
       "23                                                                                                                  blagh class at 8 tomorrow   \n",
       "24                                                                                              I hate when I have to call and wake people up   \n",
       "25                                                                           Just going to cry myself to sleep after watching Marley and Me.    \n",
       "26                                                                                                                      im sad now  Miss.Lilly  \n",
       "27                                                   ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again   \n",
       "28                                                           Meh... Almost Lover is the exception... this track gets me depressed every time.   \n",
       "29                                                                                some1 hacked my account on aim  now i have to make a new one  \n",
       "30         @alielayus I want to go to promote GEAR AND GROOVE but unfornately no ride there  I may b going to the one in Anaheim in May though  \n",
       "31         thought sleeping in was an option tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon!   \n",
       "32                                                                                    @julieebaby awe i love you too!!!! 1 am here  i miss you  \n",
       "33                                                                                           @HumpNinja I cry my asian eyes to sleep at night   \n",
       "34             ok I'm sick and spent an hour sitting in the shower cause I was too sick to stand and held back the puke like a champ. BED now   \n",
       "35                                       @cocomix04 ill tell ya the story later  not a good day and ill be workin for like three more hours...  \n",
       "36                                                                               @MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge  \n",
       "37                                  @fleurylis I don't either. Its depressing. I don't think I even want to know about the kids in suitcases.   \n",
       "38                           Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend   \n",
       "39                                             really don't feel like getting up today... but got to study to for tomorrows practical exam...   \n",
       "40                                         He's the reason for the teardrops on my guitar the only one who has enough of me to break my heart   \n",
       "41                                                   Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!  \n",
       "42                                       @JonathanRKnight Awww I soo wish I was there to see you finally comfortable! Im sad that I missed it   \n",
       "43                              Falling asleep. Just heard about that Tracy girl's body being found. How sad  My heart breaks for that family.  \n",
       "44                                             @Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...   \n",
       "45  Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?  \n",
       "46                                                                 Oh man...was ironing @jeancjumbe's fave top to wear to a meeting. Burnt it   \n",
       "47                                                                                         is strangely sad about LiLo and SamRo breaking up.   \n",
       "48                                                                         @tea oh! i'm so sorry  i didn't think about that before retweeting.  \n",
       "49           Broadband plan 'a massive broken promise' http://tinyurl.com/dcuc33 via www.diigo.com/~tautao Still waiting for broadband we are   "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flagless_dframe.head(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1,685 tweets with neutral sentiment. The amount of neutral sentiments is comparably very low, but still better nothing at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also drop all unnecessary columns for tweet sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "singledout_dframe = flagless_dframe.drop(columns=['id', 'date', 'user'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(singledout_dframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>span{background-color: transparent; color: orange;}</style><span>Thus, 'singledout_dframe' is the table we are going to use for training our ML solution</span>; half-truth, since we will need to prune the column contents quite a lot before we can use it for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=' Step 3.2'>Step 3.2.:</a> Generate Corpus</h3>\n",
    "\n",
    "Here we make one big text out of all tokens used in the 'text' column; call it 'corpus'. For concatenating all the sentences in each row of the table, we use *join* instead of *str.cat*\n",
    "<ol>\n",
    "    <li>corpus = singledout_dframe['text'].str.cat(sep=' ')</li>\n",
    "    <li>corpus = ' '.join(singledout_dframe['text'].astype(str))</li>\n",
    "</ol>\n",
    "\n",
    "Join shows to have a shorter Execution Time but a somewhat longer Overhead Time, whereas str.cat has the opposite characteristics. And to borrow from my dear companion, Chat GPT: \"In general, the *join* method may use slightly less memory than *str.cat*, especially if you use the *astype(str)* method to convert the values in the DataFrame column to strings before concatenation. This is because the *join* method creates a new string object containing the concatenated strings, whereas *str.cat* creates a new Series object containing the concatenated strings.\"\n",
    "\n",
    "Anyhow, we use the *join* method for the concatenation and save the result as a txt-file.\n",
    "\n",
    "But...\n",
    "\n",
    "Before we do that, we also generate a table, where we save each tweet on a separate row. We call it Line-By-Line Corpus or 'lbl_corpus' and save it as a txt-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./.repository/corpus.txt\n"
     ]
    }
   ],
   "source": [
    "# write a code that writes each row of singledout_dframe['text'] on a separate line to a file called lbl_corpus.txt\n",
    "with open('lbl_corpus.txt', 'w') as f:\n",
    "    for row in singledout_dframe['text']:\n",
    "        f.write(row + '\\n')\n",
    "\n",
    "# write a code that writes each row of singledout_dframe['sentiment'] on a separate line to a file called lbl_corpus_sentiment.txt\n",
    "with open('lbl_corpus_sentiment.txt', 'w') as f:\n",
    "    for row in singledout_dframe['sentiment']:\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "# make a corpus of all the tweets in singledout_dframe['text'] and write it to a file called corpus.txt\n",
    "dframe_corpus = ' '.join(singledout_dframe['text'].astype(str))\n",
    "print(const.repo_path+'corpus.txt')\n",
    "with open(const.repo_path+'corpus.txt', 'w') as f:\n",
    "    f.write(dframe_corpus)\n",
    "\n",
    "# write the size of the original corpus to a file called corpus_mensura.txt\n",
    "with open(const.repo_path+str(const.CORPUS_MENSURA), 'w') as f:\n",
    "    f.write(str(len(dframe_corpus.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily see that a character in this particular system - PowerMac Laptop 1.4 GHz Quad-Core Intel Core i5 with Ventura 13.3.1, Visual Studio Code Version: 1.77.3 (Universal), Jupyter Notebook 6.4.12 - is 1 byte long but with a overhead of 49 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The string \"T\" is {len(\"T\")} characters long')\n",
    "# print(f'But it occupies a total of {pkg.sys.getsizeof(\"T\")} bytes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we expect the whole 'corpus', which is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{len(dframe_corpus)} characters long')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 119980390 + 49"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bytes of memory, but obviously it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg.sys.getsizeof(dframe_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bytes. This yields that the overhead for this corpus is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (119980463-119980390)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bytes long."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id='sidetrackmemoryusage'>Side Track:</a> Memory Usage</h3>\n",
    "\n",
    "To investigate the reason for the overhead and its content we install and use \"memory_profiler\" which is a module for monitoring memory usage of a python program. We use it to see how much memory is used by each line of code in the .py-copy of this very notebook. The result is depicted in the following graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg.Image(filename=const.repo_path+'memory_time-space_profile.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a breakdown of the memory usage we utilize the function objgraph.show_backrefs, that recrusively shows all objects that have a reference to the input 'corpus' variable. The result is illustrated in the graph that follows. There, we see a that the str object 'corpus' occpies 119,980,463 bytes of memory, which is the same number we got from the *sys.getsizeof* function. This means that we still have no idea what these extra 73 bytes are and where they come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg.objgraph.show_backrefs(dframe_corpus, max_depth=4, too_many=5, filename=const.repo_path+'/corpus_backrefs_4-5.png', extra_info=lambda x: f\"{pkg.sys.getsizeof(x)} bytes\")\n",
    "# pkg.Image(filename=const.repo_path+'/corpus_backrefs_4-5.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, back to the main track...\n",
    "\n",
    "<h3><a id=' Step 3.3'>Step 3.3.:</a> Memory Space Clean Out</h3>\n",
    "\n",
    "To clean out the memory space, we delete the 'dframe_corpus' variable along with all the other dataframe objects we have created so far. We have already saved it as a txt-file. Also, the singledout_dframe is saved as a csv-file, so it's save to remove. This way, we'll save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total memory space that can be released is about is 2.60 GB\n",
      "The total memory occupied by the varibles in the program is 2.60 GB\n"
     ]
    }
   ],
   "source": [
    "import functions as func\n",
    "\n",
    "keys_to_remove = [key for key, value in locals().items() if not key.startswith('_') and 'dframe' in key]\n",
    "print(f'The total memory space that can be released is about {(func.total_occupied_space(locals(), keys_to_remove))}')\n",
    "print(f'The total memory occupied by the varibles in the program {(func.total_occupied_space(locals(), locals().keys()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following variables are begin deleted:\n",
      "['dframe', 'grouped_dframe', 'sorted_dframe', 'largest_dframe', 'all_dframe_ids', 'duplicated_dframe_ids', 'duplicated_tweet_dframe', 'dframe_wihout_duplicates', 'neutralized_dframe', 'flagless_dframe', 'singledout_dframe', 'dframe_corpus']\n"
     ]
    }
   ],
   "source": [
    "keys_to_remove = list([key for key, value in locals().items() if not key.startswith('_') and 'dframe' in key])\n",
    "func.cleanup_variable_space(locals(), keys_to_remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=' Step 3.4'>Step 3.4.:</a> The Vocabulary</h3>\n",
    "\n",
    "In every database there is a vocabulary, which is a list of all the words that are used in the database. The amount of words may create a problem for the ML algorithm, since it has to learn the weights of all the words in the vocabulary. In our case, the vocabulary is the set of all the words that are used in the 'text' column of the 'singledout_dframe' dataframe. We can easily get it by using the *set* function on the 'corpus' variable. In the following substeps, we will try to reduce the size of the vocabulary by removing all the words that are not relevant for the sentiment classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><a id=' Step 3.4.1.'>Step 3.4.1.:</a> Remove Unnecessary Tokens</h5>\n",
    "\n",
    "By *token* we mean any cohesive series of characters except the *space* character. In our original corpus, we have 21,052,251 tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are supposed to begin with the removal of hyperlinks, @- and #-tags, other punctuation and stopwords... but... having emoticons in the text enriches its content and makes it more expressive. So, we should protect these tokens from the removal. We will do it in the next step. Let us start with finding out about existence of emoticons in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions as func\n",
    "\n",
    "# global print\n",
    "# print = pkg.functools.partial(print, flush=True)\n",
    "\n",
    "# #func.list_affected_tokens('corpus', {':)': 'smiling face', ':(': 'sad face', ':\\'(': 'crying face'}, 'mix')\n",
    "# func.list_affected_tokens('corpus', const.emoticons, 'mix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, experimenting with these emoticons shows that we should be mindful of the precedence of the more complicated ones. For instance, 'http:/' that is part of the well-known hyperlink, should be removed before we count emoticons. After all, we wouldn't want to keep 'http:/' as a skeptical emoticon \":/\", would we? So, let us remove all hyperlinks first.\n",
    "\n",
    "What is important to consider is that for every string we try to remove, there will be manyfold variations. For example, the http may be found as http in all hyperlinks. But, where only the word HTTP is used, it will not be found by our algorithms, unless for every word in the corpus we check for all the possible variations. We also know that the variation of the fonts, being small or capital letters won't affect their sentiment. Thus, we have to make sure the whole text is converted to lower case before we start removing any tokens.\n",
    "\n",
    "We do it right here, right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "pkg.shutil.copyfile(const.repo_path+'corpus.txt', const.repo_path+'corpus_original.txt')\n",
    "all_small = ''\n",
    "with open(const.repo_path+'corpus.txt', 'r') as f:\n",
    "    all_small = f.read().lower()\n",
    "with open(const.repo_path+'corpus.txt', 'w') as g:\n",
    "    g.write(all_small)\n",
    "del all_small\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><a id=' Step 3.4.1.1.'>Step 3.4.1.1.</a> Remove Hyperlinks</h5>\n",
    "\n",
    "First, let's see how many URLs - or any substrings as 'http', 'ftp', 'ssh', and so on that may contain or be about hyperlinks - there are in the corpus. We write them all to the file 'urls.txt' and count the number of lines in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signum =  http\n",
      "There are 71,659 tokens in './.repository/signa_continentur_http.txt' containing 'http'\n",
      "The previous corpus had ('21052251', ',') tokens and the new one has\n",
      " 20,980,592; this is 71,659 less.\n",
      "The previous corpus had ('21052251', ',') tokens and the new one has 20,980,592; this is 71,659 less.\n",
      "Writing the new, reduced corpus to './.repository/corpus_sine_http.txt' ...\n"
     ]
    }
   ],
   "source": [
    "func.string_affected_tokens('corpus', 'http', 'signa_continentur_http')\n",
    "no_http = func.remove_token('corpus', 'http', 'corpus_sine_http')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also check for other means of file-transport like ftp and ssh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signum =  ftp\n",
      "There are 107 tokens in './.repository/signa_continentur_ftp.txt' containing 'ftp'\n",
      "signum =  ssh\n",
      "There are 1,673 tokens in './.repository/signa_continentur_ssh.txt' containing 'ssh'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functions as func\n",
    "\n",
    "func.string_affected_tokens('corpus', 'ftp', 'signa_continentur_ftp')\n",
    "func.string_affected_tokens('corpus', 'ssh', 'signa_continentur_ssh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there were 'ftp's that can be removed but no 'ssh' in the sense we mean, but they were all parts of names. So we leave them as they are and only remove the ftp-related tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous corpus had ('20980592', ',') tokens and the new one has\n",
      " 20,980,489; this is 103 less.\n",
      "The previous corpus had ('20980592', ',') tokens and the new one has 20,980,489; this is 103 less.\n",
      "Writing the new, reduced corpus to './.repository/corpus_sine_ftp.txt' ...\n"
     ]
    }
   ],
   "source": [
    "no_ftp = func.remove_token(func.strip_file_name(no_http), 'ftp', 'corpus_sine_ftp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the @ sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signum =  @\n",
      "There are 797,271 tokens in './.repository/signa_continentur_@.txt' containing '@'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.string_affected_tokens('corpus', '@', 'signa_continentur_@')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's natural to have so many of this sign in Twitter. We remove them all, since they won't affect the sentiment classification anyhow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous corpus had ('20980489', ',') tokens and the new one has\n",
      " 20,183,282; this is 797,207 less.\n",
      "The previous corpus had ('20980489', ',') tokens and the new one has 20,183,282; this is 797,207 less.\n",
      "Writing the new, reduced corpus to './.repository/signa_continentur_@.txt' ...\n"
     ]
    }
   ],
   "source": [
    "no_at = func.remove_token(func.strip_file_name(no_ftp), '@', 'signa_continentur_@')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at tokens containing #-tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signum =  #\n",
      "There are 44,986 tokens in './.repository/signa_continentur_#.txt' containing '#'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.string_affected_tokens('corpus', '#', 'signa_continentur_#')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are also very common in Twitter, and some of them, like #MakesMeSmile can convey a sentiment. So, we leave them as they are. Hopefully coming strategies will take care of them.\n",
    "\n",
    "So, to cleaning punctuation:\n",
    "\n",
    "Since the punctuation marks are not part of the vocabulary, we can remove them but they overlap a lot with the emoticons. So, we will remove them after we have dealt with the emoticons. Let us first all emoticons and replace them with their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the new, reduced corpus to './.repository/clavis_valorem_commutationem.txt' ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./.repository/clavis_valorem_commutationem.txt'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.key_value_exchange(func.strip_file_name(no_at), const.emoticons, 'clavis_valorem_commutationem')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I guess we can remove the punctuation marks, or actually, every character that is not a letter or a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 9,470,402 characters that match the regex '[^a-zA-Z]'.\n",
      "This is 8.66% of the whole corpus.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.count_regex(func.strip_file_name(no_at), r'[^a-zA-Z]', 'signa_continentur_non_literas')\n",
    "# no_non_letters = func.remove_token(func.strip_file_name(no_at), r'[^a-zA-Z]', 'corpus_sine_non_literas')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><a id=' Step 3.1.1.2.'>Step 3.1.1.2.</a> Remove @tags</h5>\n",
    "\n",
    "Next, let's see how many @tags there are in the corpus. We write them all to the file 'at_signis.txt' and count the number of lines in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(const.repo_path+'/corpus_sine_delata.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "#     at_signis = pkg.re.findall(r'\\S*@\\S*', text)\n",
    "#     print(f'There are {len(at_signis)} @-related tokens in the http free corpus')\n",
    "#     with open(const.repo_path+'/at_signis.txt', 'w') as f:\n",
    "#         for ad_signum in at_signis:\n",
    "#             f.write(ad_signum + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yet another 797,227 tokens are identified in the corpus and saved in the file 'ad_signis.txt', making them 3.79% of the tokens in the original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7868967076252322"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "797227/21052251*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing them from the corpus_sine_delata.txt file, we get the corpus_sine_signo.txt file, which has 20,183,438 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_sine_signo = pkg.re.sub(r'\\S*@\\S*', '', corpus_sine_delata)\n",
    "# print(f'Original corpus has {len(dframe_corpus.split())} tokens and the new corpus has {len(corpus_sine_signo.split())} tokens')\n",
    "# with open(const.repo_path+'/corpus_sine_signo.txt', 'w') as f:\n",
    "#     f.write(corpus_sine_signo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5><a id=' Step 3.1.1.3.'>Step 3.1.1.3.</a> Remove #-tags</h5>\n",
    "\n",
    " We do the same for the hashtags '#'. First we same the those tokens containing the '#' character to the file 'nullam_marcam.txt', and then we remove them from the corpus_sine_signo.txt file, which we save as 'sine_corpore_nullam.txt'. The result is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(const.repo_path+'/corpus_sine_signo.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "#     nullam_marcas = pkg.re.findall(r'\\S*#\\S*', text)\n",
    "#     print(f'There are {len(nullam_marcas)} #-related tokens in the hyperlink-and-@-free corpus')\n",
    "#     with open(const.repo_path+'/nullam_marcas.txt', 'w') as f:\n",
    "#         for nullam_marcam in nullam_marcas:\n",
    "#             f.write(nullam_marcam + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing even the hash tags with 44,702 related tokens, we get a corpus which is free from hyperlinks, hashtags, and @signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_liberum_nullam = pkg.re.sub(r'\\S*#\\S*', '', corpus_sine_signo)\n",
    "# print(f'Original corpus has {len(dframe_corpus.split())} tokens and the new corpus has {len(corpus_liberum_nullam.split())} tokens')\n",
    "# with open(const.repo_path+'/corpus_liberum_nullam.txt', 'w') as f:\n",
    "#     f.write(corpus_liberum_nullam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The '@' involves only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21233833854631506"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "44702/21052251*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.21% of the tokens in the original corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.1. COUNT SMILIES IN THE CORPUS... THEY CAN HELP WITH THE SENTIMENT CLASSIFICATION...\n",
    "\n",
    "These are the most common smilies used in Twitter:\n",
    "\n",
    ":) - Smiling face\n",
    "\n",
    ":-) - Smiling face\n",
    "\n",
    ";) - Winking face\n",
    "\n",
    ";-) - Winking face\n",
    "\n",
    ":( - Frowning face\n",
    "\n",
    ":-( - Frowning face\n",
    "\n",
    ":D - Grinning face\n",
    "\n",
    ":d - Grinning face\n",
    "\n",
    ":-D - Grinning face\n",
    "\n",
    ":-d - Grinning face\n",
    "\n",
    ":P - Sticking out tongue\n",
    "\n",
    ":p - Sticking out tongue\n",
    "\n",
    ":-P - Sticking out tongue\n",
    "\n",
    ":-p - Sticking out tongue\n",
    "\n",
    ":-o - Shocked face\n",
    "\n",
    ":-O - Shocked face\n",
    "\n",
    ":o - Shocked face\n",
    "\n",
    ":O - Shocked face\n",
    "\n",
    ":-| - Neutral face\n",
    "\n",
    ":| - Neutral face\n",
    "\n",
    ":-* - Kiss\n",
    "\n",
    ":* - Kiss\n",
    "\n",
    ":/ - Skeptical face\n",
    "\n",
    ":-/ - Skeptical face\n",
    "\n",
    "<3 - Heart\n",
    "\n",
    "</3 - Broken heart\n",
    "\n",
    "____________ [Step 3.3.1.1.:](#step3.3.1.1.) Remove URLs\n",
    "\n",
    "____________ [Step 3.3.1.2.:](#step3.3.1.2.) Remove @-tags\n",
    "\n",
    "____________ [Step 3.3.1.2.:](#step3.3.1.3.) Remove #-tags\n",
    "\n",
    "____________ [Step 3.3.1.3.:](#step3.3.1.4.) Remove punctuation\n",
    "\n",
    "So we need to find if we have these strings in the corpus and get the statistics of their usage.\n",
    "\n",
    "2.2. THERE MUST BE A LIST OF MOST FREQUENTLY USED SMILIES IN TWTTER. I THINK THERE IS DICTIONARY OF TWITTER SIMILIES SOMEWHERE ON THE INTERNET\n",
    "\n",
    "2.3. LOOK AT ALL TUPLES OF SPECIAL CHARACTERS THAT ARE OF LENGTH 3\n",
    "\n",
    "2.4. LOOK AT ALL TUPLES OF SPECIAL CHARACTERS THAT ARE OF LENGTH 2\n",
    "\n",
    "3. FIND OTHER SPECIAL CHARACTERS THAT MAY HAVE BEEN USED IN THE CORPUS\n",
    "\n",
    "4. SEE IF YOU CAN FIND ANY ANOMALIES IN THE CORPUS THAT STILL CAN AFFECT THE SENTIMENT CLASSIFICATION\n",
    "\n",
    "5. FIND ALL NON-LETTER CHARACTERS THAT ARE USED FOR BUILDING TOKENS, WHICH ARE REPEATED MORE THAN ONCE\n",
    "\n",
    "____________ [Step 3.3.1.4.:](#step3.3.1.4.) Remove \"stop\" words\n",
    "\n",
    "________________ [Step 3.3.1.4.1.:](#step3.3.1.4.1.) Articles: \"a\", \"an\", \"the\"\n",
    "\n",
    "________________ [Step 3.3.1.4.2.:](#step3.3.1.4.2.) Conjunctions: \"and\", \"or\", \"but\"\n",
    "\n",
    "________________ [Step 3.3.1.4.3.:](#step3.3.1.4.3.) Prepositions: \"at\", \"on\", \"in\",\"of\", \"to\", \"with\", \"by\"\n",
    "\n",
    "________________ [Step 3.3.1.4.4.:](#step3.3.1.4.4.) Pronouns: \"he\", \"she\", \"it\", \"they\", \"we\", \"you\"\n",
    "\n",
    "________________ [Step 3.3.1.4.5.:](#step3.3.1.4.5.) Auxiliary verbs: \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"have\", \"has\", \"had\", \"do\", \n",
    "\"does\", \"did\"\n",
    "\n",
    "________________ [Step 3.3.1.4.6.:](#step3.3.1.4.6.) Adverbs of frequency: \"always\", \"usually\", \"often\", \"sometimes\", \"rarely\", \"never\"\n",
    "\n",
    "________________ [Step 3.3.1.4.7.:](#step3.3.1.4.7.) Interjections: \"oh\", \"ah\", \"wow\", \"hmm\"\n",
    "\n",
    "________________ [Step 3.3.1.4.8.:](#step3.3.1.4.8.) Suggestions from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x2 = func.remove_token(func.strip_file_name(x1), 'www', 'corpus_sine_www')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x3 = func.get_affected_tokens(func.strip_file_name(x2), '@', 'z_test_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a id=' Step 3.3.2'>Step 3.3.2.:</a> Gather Unique Tokens</h4>\n",
    "\n",
    "\n",
    "<h4><a id=' Step 3.3.3'>Step 3.3.3.:</a> Split & Count</h4>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>UNDER CONSTRUCTION</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Python package 'collections' to create a set of tuples with each unique token and its frequency in the 'corpus'. We choose to sort the token in descending order of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_counts = pkg.Counter(dframe_corpus.split())\n",
    "# print(token_counts.most_common(10))  # print the 20 most common tokens\n",
    "# # Sort the tokens and their frequencies in descending order\n",
    "# sorted_counts = sorted(token_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "# # Print the 20 most common tokens\n",
    "# print(sorted_counts[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write the result to a CSV file that we call 'token_frequencies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the sorted counts to a CSV file\n",
    "# with open('token_frequencies.csv', 'w', newline='\\n') as csvfile:\n",
    "#     writer = pkg.csv.writer(csvfile)\n",
    "#     writer.writerow(['Token', 'Count'])  # write the header row\n",
    "#     for token, count in sorted_counts:\n",
    "#         writer.writerow([token, count])\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a small word cloud of what is left of the tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the word frequencies from a CSV file\n",
    "# word_freqs = {}\n",
    "# with open('token_frequencies.csv', 'r') as file:\n",
    "#     reader = pkg.csv.reader(file)\n",
    "#     new_header = next(reader)  # skip the header row\n",
    "#     for row in reader:\n",
    "#         # if the word frequency is greater than 26336, add it to the dictionary\n",
    "#         if int(row[1]) > 26336.:\n",
    "#             word_freqs[row[0]] = int(row[1])\n",
    "\n",
    "# # Create the tags for the word cloud\n",
    "# tags = pkg.make_tags([(word, 1.) for word in word_freqs.keys()], maxsize=80)\n",
    "\n",
    "# # Set the tag sizes based on the word frequencies\n",
    "# for tag in tags:\n",
    "#     tag['size'] = int(word_freqs[tag['tag']] / max(word_freqs.values()) * 100)\n",
    "\n",
    "# # Generate the image for the word cloud\n",
    "# pkg.create_tag_image(tags, 'wordcloud_gt-26336.png', size=(600, 400), fontname='Lobster')\n",
    "\n",
    "# pkg.Image(filename='./wordcloud_gt-26336.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=' Step 3'>Step 3.:</a> Cleaning with Regard to Sentiments</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id='step4'>Step 4.:</a> Learn from Google's Search Engine</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove all unnecessary words from the corpus and find\n",
    "2. Build dictionary\n",
    "3. Code the words and phrases\n",
    "4. Run ML algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a id='sidetracksplitthedate'>Side Track</a> - Split the Date</h1>\n",
    "\n",
    "Divide the 'date' into separate columns with the names 'day name', 'year', 'month', 'day', 'hour', 'minute', 'second'**\n",
    "\n",
    "Let us see how many time zones are mentioned in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flagless_dframe['date'].str.extract(r'([A-Z]{3})', expand=False).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we only have PDT as the time zone. We can thus skip this by removing it from all the strings in the 'date' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoneless_dframe = flagless_dframe.copy()\n",
    "# zoneless_dframe['date'] = zoneless_dframe['date'].str.replace('PDT', '')\n",
    "# zoneless_dframe.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can divide the date up into 6 different columns, for other investigations than the main subject of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated_dframe = zoneless_dframe.copy()\n",
    "# # Convert the 'date' column to a datetime type\n",
    "# separated_dframe['date'] =pkg.pd.to_datetime(separated_dframe['date'], format='%a %b %d %H:%M:%S %Y')\n",
    "# # Extract individual date components into separate columns\n",
    "# separated_dframe['year'] = separated_dframe['date'].dt.year\n",
    "# separated_dframe['month'] = separated_dframe['date'].dt.month\n",
    "# separated_dframe['day'] = separated_dframe['date'].dt.day\n",
    "# separated_dframe['hour'] = separated_dframe['date'].dt.hour\n",
    "# separated_dframe['minute'] = separated_dframe['date'].dt.minute\n",
    "# separated_dframe['second'] = separated_dframe['date'].dt.second\n",
    "# separated_dframe['weekday'] = separated_dframe['date'].dt.weekday\n",
    "# # Drop the original 'date' column\n",
    "# dateless_dframe = separated_dframe.drop('date', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show the neutralized, zoneless dframe with separated date details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dateless_dframe.sort_values(by='weekday', ascending=True).head(1599999)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the result, sorted by 'id' in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dateless_dframe.sort_values(by='id', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a id='sidetrackfollowup'>Side Track</a> Follow-Up:</h1>\n",
    "\n",
    "**TODO:** Find the ten dates when the most tweets were generated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Find out the change in the frequnency of tweets per day**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : See if there is a correlation between the sentiment of the tweet and the day it was made**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : See if there is a correlation between the sentiment of the tweet and the time of the day it was made; eg if the tweets tens to be more negative at night compared to days and so on**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='testzone'><h1>TEST ZONE</H1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the row with the id number 1467811372\n",
    "# dframe.loc[dframe['id'] == 1467813782]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a sample dataframe\n",
    "# daf =pkg.pd.DataFrame({'col1': [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]})\n",
    "# display(daf)\n",
    "# # count the occurrences of each value in the 'col1' column\n",
    "# value_counts = daf['col1'].value_counts()\n",
    "# display(value_counts)\n",
    "# # filter the result to include only values that appear three times or more\n",
    "# repeated_values = value_counts[value_counts >= 3]\n",
    "# display(repeated_values)\n",
    "# # print the repeated values\n",
    "# print(repeated_values.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pkg_resources\n",
    "# print(pkg_resources.resource_filename('graphviz', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pydot\n",
    "\n",
    "# def references_graph(obj, max_depth=3, too_many=10):\n",
    "#     edges = []\n",
    "#     nodes = set()\n",
    "#     nodeid = 0\n",
    "\n",
    "#     def get_node(obj, depth):\n",
    "#         nonlocal nodeid\n",
    "#         if depth == 0:\n",
    "#             return None\n",
    "#         if id(obj) in nodes:\n",
    "#             return str(id(obj))\n",
    "#         if len(nodes) >= too_many:\n",
    "#             return None\n",
    "\n",
    "#         node_name = repr(obj)[:30]\n",
    "#         nodes.add(id(obj))\n",
    "#         node = pydot.Node(str(nodeid), label=node_name, shape='box')\n",
    "#         nodeid += 1\n",
    "\n",
    "#         for ref in gc.get_referents(obj):\n",
    "#             child_node = get_node(ref, depth - 1.)\n",
    "#             if child_node is not None:\n",
    "#                 edges.append((node, child_node))\n",
    "\n",
    "#         return node\n",
    "\n",
    "#     get_node(obj, max_depth)\n",
    "#     graph = pydot.graph_from_edges(edges, directed=True)\n",
    "#     graph.write_png('corpus_backrefs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format(9404204284, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbr = format(930850284, ',')\n",
    "# print(f'{format(930850284, \",\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(locals())\n",
    "# print([name for name in locals()])\n",
    "# all_names = [type(name) for name in locals().keys()]\n",
    "# index = 0\n",
    "# for name, content in globals().items():\n",
    "#     print('<---------------------------- ', index,' -------------------------------------->')\n",
    "#     index += 1\n",
    "#     print(f'{name} : {content}')\n",
    "# #    print(f'{name} = {locals()[name]}')\n",
    "#     if index > 27:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def bubble_sort(arr):\n",
    "#     n = len(arr)\n",
    "#     for i in range(n):\n",
    "#         for j in range(0, n-i-1):\n",
    "#             if arr[j] > arr[j+1] :\n",
    "#                 arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     arr = [random.randint(1, 100) for _ in range(1000)]\n",
    "#     bubble_sort(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_var_name(var):\n",
    "#     \"\"\"Return the name of a variable as a string.\"\"\"\n",
    "#     for name in globals():\n",
    "#         print(name)\n",
    "#         if id(globals()[name]) == id(var):\n",
    "#             return name\n",
    "#     return None\n",
    "\n",
    "# # Example usage\n",
    "# x = 42\n",
    "# y = 'hello'\n",
    "# print(get_var_name(x)) # Output: 'x'\n",
    "# print(get_var_name(y)) # Output: 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func.my_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from varname.helpers import Wrapper\n",
    "\n",
    "# foo = Wrapper(dict())\n",
    "\n",
    "# # foo.name == 'foo'\n",
    "# # foo.value == {}\n",
    "# foo.value['bar'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from varname import varname\n",
    "# def function():\n",
    "#     return varname()\n",
    "\n",
    "# func = function()  # func == 'func'\n",
    "# print(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function():\n",
    "#     # retrieve the variable name at the 2nd frame from this one\n",
    "#     return varname(frame=1)\n",
    "\n",
    "# func = function()  # func == 'func'\n",
    "# print(varname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wrapped():\n",
    "#     print(varname(frame=1))\n",
    "#     return function()\n",
    "\n",
    "# def function():\n",
    "#     print(varname(frame=2))\n",
    "#     # retrieve the variable name at the 2nd frame from this one\n",
    "#     return function_()\n",
    "\n",
    "# def function_():\n",
    "#     print(varname(frame=3))\n",
    "#     holder = 'string'\n",
    "#     return holder.varname(frame=4)\n",
    "\n",
    "# func = wrapped() # func == 'func'\n",
    "# print(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since v0.5.4\n",
    "# def func():\n",
    "#     return varname(multi_vars=True)\n",
    "\n",
    "# a = func() # a == ('a',)\n",
    "# a, b = func() # (a, b) == ('a', 'b')\n",
    "# [a, b] = func() # (a, b) == ('a', 'b')\n",
    "\n",
    "# # hierarchy is also possible\n",
    "# a, (b, c) = func() # (a, b, c) == ('a', 'b', 'c')\n",
    "# print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from varname.helpers import register\n",
    "\n",
    "# @register\n",
    "# def function():\n",
    "#     koja = 'koja'\n",
    "#     return koja.__varname__\n",
    "\n",
    "# func = function() # func == 'func'\n",
    "\n",
    "# print(func)\n",
    "# # @register(frame=2)\n",
    "# # def function():\n",
    "# #     return __varname__\n",
    "\n",
    "# # def wrapped():\n",
    "# #     return function()\n",
    "\n",
    "# # func = wrapped() # func == 'func'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from varname import varname, nameof\n",
    "\n",
    "# p = 12\n",
    "# x = p\n",
    "# f = nameof('hasanali') # 'varname'\n",
    "# print(nameof(func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from varname import varname\n",
    "\n",
    "# def func():\n",
    "#   return varname()\n",
    "\n",
    "# # In external uses\n",
    "# x = func() # 'x'\n",
    "# y = func() # 'y'\n",
    "# print(x, y)\n",
    "# print(nameof(x))\n",
    "# print(nameof(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# my_dataframe = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': ['a', 'b', 'c']})\n",
    "\n",
    "# def process_dataframe(dataframe_name):\n",
    "#     # retrieve the DataFrame object from its name using the globals() function\n",
    "#     print(type(dataframe_name))\n",
    "#     df = globals()[dataframe_name]\n",
    "#     print(type(df))\n",
    "#     print(df)\n",
    "#     # do something with the DataFrame object\n",
    "#     # ...\n",
    "\n",
    "# process_dataframe('my_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# # Get a list of all running processes\n",
    "# processes = psutil.process_iter()\n",
    "\n",
    "# # Iterate over the list of processes and find the one you want to kill\n",
    "# for process in processes:\n",
    "#     print(process.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# ___aaa = dframe_corpus\n",
    "# print(sys.getsizeof(___aaa))\n",
    "# print(locals().keys())\n",
    "# if '___aaa' in globals().keys():\n",
    "#     print('___aaa is')\n",
    "# if 'dframe_corpus' in globals().keys():\n",
    "#     print('dframe_corpus is')\n",
    "# ___ninanoo = ___aaa\n",
    "# del ___aaa\n",
    "# del dframe_corpus\n",
    "# gc.collect()\n",
    "# if '___aaa' in globals().keys():\n",
    "#     print('___aaa is')\n",
    "# if 'dframe_corpus' in globals().keys():\n",
    "#     print('dframe_corpus is')\n",
    "# print(locals().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http is not written liek zis'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "'Http is not writTen liek Zis'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9e79cc4e328e53239abfbe265d0053d0932c82df73543d05f8bfb6c40e2fe6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
